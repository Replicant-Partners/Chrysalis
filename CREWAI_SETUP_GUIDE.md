# How to Set Up and Execute Chrysalis CrewAI Agent Teams

**Date**: December 28, 2025  
**Purpose**: Step-by-step guide to translate YAML plans into running CrewAI agent crews  
**Prerequisites**: CrewAI installed, Python 3.10+, OpenAI API key or local LLM

---

## Overview

This guide explains how to:
1. Set up CrewAI environment
2. Create agent crews from YAML specifications
3. Execute teams in coordinated phases
4. Monitor progress and manage dependencies
5. Handle handoffs between teams

---

## Prerequisites

### 1. Install CrewAI

```bash
pip install crewai crewai-tools

# Or with specific version
pip install 'crewai>=0.28.0' 'crewai-tools>=0.2.0'
```

### 2. Set Up LLM

**Option A: OpenAI (Recommended for quality)**:
```bash
export OPENAI_API_KEY='your-api-key-here'
export OPENAI_MODEL='gpt-4'  # or gpt-4-turbo for speed
```

**Option B: Local LLM (LM Studio, Ollama)**:
```bash
# LM Studio
export OPENAI_API_BASE='http://localhost:1234/v1'
export OPENAI_API_KEY='lm-studio'

# Ollama
export OLLAMA_MODEL='llama2'
```

**Option C: Anthropic Claude**:
```bash
export ANTHROPIC_API_KEY='your-api-key'
# Configure CrewAI to use Anthropic
```

### 3. Clone Chrysalis Repository

```bash
git clone https://github.com/Replicant-Partners/Chrysalis.git
cd Chrysalis
```

### 4. Create CrewAI Project Structure

```bash
mkdir -p crews/{team1_core,team2_security,team3_infra,team4_ml,team5_research}
mkdir -p crews/shared/{tools,utils,templates}
```

---

## Creating Agent Crews from Plans

### Team 1: Core Platform Engineering

**Create**: `crews/team1_core/crew.py`

```python
from crewai import Agent, Task, Crew, Process
from crewai_tools import FileReadTool, FileWriteTool, CodeInterpreterTool

# ============================================================================
# AGENTS
# ============================================================================

platform_architect = Agent(
    role="Lead Platform Architect",
    goal="""Design and oversee implementation of Chrysalis core platform ensuring
    fractal composition principles, adaptive pattern resolution, and lossless
    agent morphing using open source distributed systems patterns.""",
    backstory="""Senior architect with 15+ years in distributed systems, specializing in
    agent architectures and framework interoperability. Deep expertise in
    design patterns, SOLID principles, and open source ecosystems. Champions
    evidence-based design and fractal composition. Previously architected
    large-scale multi-agent systems at research institutions.""",
    tools=[
        FileReadTool(),
        FileWriteTool(),
        # Add custom architecture tools
    ],
    verbose=True,
    allow_delegation=True,
    memory=True
)

backend_engineer = Agent(
    role="Senior Backend Engineer",
    goal="""Implement Chrysalis core modules (PatternResolver, framework adapters,
    experience sync, state merging) with production-grade code quality,
    comprehensive testing, and performance optimization.""",
    backstory="""Expert TypeScript engineer with 10+ years building distributed systems.
    Specialist in async/await patterns, event-driven architectures, and
    high-performance Node.js applications. Strong advocate for test-driven
    development, clean code, and open source best practices.""",
    tools=[
        FileReadTool(),
        FileWriteTool(),
        CodeInterpreterTool(),
        # Add custom dev tools
    ],
    verbose=True,
    allow_delegation=False,
    memory=True
)

# ... define other 3 agents (devex_engineer, integration_engineer, qa_engineer)

# ============================================================================
# TASKS
# ============================================================================

task_1_1_1 = Task(
    description="""Design the AdaptivePatternResolver architecture. Create comprehensive
    specification covering:
    - Interface design (PatternResolution<T>, DeploymentContext)
    - Strategy pattern implementation
    - Context evaluation logic (distributed, performance-critical, MCP available)
    - Graceful degradation strategy
    - Performance monitoring approach
    - Mermaid diagrams showing data flow
    
    Read context from: CHRYSALIS_UNIFIED_SPEC_V3.1.md, SYNTHESIS_REPORT_FINAL.md""",
    expected_output="""Comprehensive design document (20+ pages) including:
    - PatternResolver.ts interface specifications
    - 5 pattern implementation interfaces
    - Context evaluation decision tree
    - Mermaid sequence diagrams (3+)
    - Performance requirements (<0.5ms overhead)
    - Error handling strategy""",
    agent=platform_architect,
    context=[],  # Will add file contexts
    output_file="design/PatternResolver-Design.md"
)

task_1_1_2 = Task(
    description="""Review and enhance UniformSemanticAgentV2 TypeScript interface...""",
    expected_output="""Enhanced schema specification...""",
    agent=platform_architect,
    context=[],
    output_file="design/UniformSemanticAgentV2-Schema.md"
)

# ... define all tasks from TEAM1_CORE_PLATFORM.yaml

# ============================================================================
# CREW
# ============================================================================

team1_crew = Crew(
    agents=[
        platform_architect,
        backend_engineer,
        devex_engineer,
        integration_engineer,
        qa_engineer
    ],
    tasks=[
        task_1_1_1,
        task_1_1_2,
        task_1_1_3,
        task_1_2_1,
        task_1_2_2,
        # ... all tasks in sequence
    ],
    process=Process.SEQUENTIAL,
    verbose=True,
    memory=True,
    cache=True,
    planning=True
)

# ============================================================================
# EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("ðŸ¦‹ Starting Chrysalis Team 1: Core Platform Engineering")
    print("=" * 60)
    
    # Read context files
    import os
    
    spec_path = "../../CHRYSALIS_UNIFIED_SPEC_V3.1.md"
    if os.path.exists(spec_path):
        with open(spec_path) as f:
            spec_content = f.read()
        # Add to task context
    
    # Execute crew
    result = team1_crew.kickoff()
    
    print("\n" + "=" * 60)
    print("âœ… Team 1 Execution Complete")
    print(f"Result: {result}")
```

### Team 2-5: Similar Structure

For each team, create `crews/teamN_*/crew.py` following same pattern:
1. Define agents from YAML spec
2. Define tasks from YAML spec
3. Configure crew
4. Add execution logic

**Templates Available**: `crews/shared/templates/crew_template.py`

---

## Execution Modes

### Mode 1: Sequential (Single Team)

Execute one team at a time, wait for completion before next:

```bash
cd crews/team1_core
python crew.py  # Run Team 1 (6 weeks of work)

# Wait for completion, review output

cd ../team2_security
python crew.py  # Run Team 2 (parallel with Team 1 in practice)
```

**Pros**: Simple, clear, easy to debug  
**Cons**: Slow (26 weeks serial), misses parallelism

### Mode 2: Parallel (Multiple Teams)

Run Teams 1, 2, 3 simultaneously for Phase 1:

**Terminal 1**:
```bash
cd crews/team1_core && python crew.py
```

**Terminal 2**:
```bash
cd crews/team2_security && python crew.py
```

**Terminal 3**:
```bash
cd crews/team3_infra && python crew.py
```

**Pros**: True parallelism, faster (6 weeks for Phase 1)  
**Cons**: More complex, need to manage 3 processes

### Mode 3: Orchestrated (Recommended)

Use orchestrator script to manage all teams:

**Create**: `crews/orchestrator.py`

```python
import subprocess
import concurrent.futures
from datetime import datetime

class CrewOrchestrator:
    def __init__(self):
        self.teams = {
            'team1': 'crews/team1_core/crew.py',
            'team2': 'crews/team2_security/crew.py',
            'team3': 'crews/team3_infra/crew.py',
            'team4': 'crews/team4_ml/crew.py',
            'team5': 'crews/team5_research/crew.py',
        }
        self.phase1 = ['team1', 'team2', 'team3']
        self.phase2 = ['team4']
        self.phase3 = ['team5']
    
    def run_team(self, team_name):
        print(f"[{datetime.now()}] Starting {team_name}...")
        result = subprocess.run(['python', self.teams[team_name]], 
                                capture_output=True, text=True)
        return team_name, result
    
    def execute_phase1(self):
        print("ðŸ¦‹ PHASE 1: Production Foundations")
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(self.run_team, team) for team in self.phase1]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]
        
        print("âœ… Phase 1 Complete")
        return results
    
    def execute_phase2(self):
        print("ðŸ¦‹ PHASE 2: Scale & Performance")
        result = self.run_team('team4')
        print("âœ… Phase 2 Complete")
        return result
    
    def execute_phase3(self):
        print("ðŸ¦‹ PHASE 3: Research & Validation")
        result = self.run_team('team5')
        print("âœ… Phase 3 Complete")
        return result
    
    def execute_all(self):
        phase1_results = self.execute_phase1()
        phase2_results = self.execute_phase2()
        phase3_results = self.execute_phase3()
        
        print("\nðŸŽ‰ ALL PHASES COMPLETE")
        return {
            'phase1': phase1_results,
            'phase2': phase2_results,
            'phase3': phase3_results
        }

if __name__ == "__main__":
    orchestrator = CrewOrchestrator()
    results = orchestrator.execute_all()
```

**Execute**:
```bash
python crews/orchestrator.py
```

**Pros**: Full automation, correct phasing, managed dependencies  
**Cons**: Long execution (26 weeks of agent work)

---

## Monitoring Execution

### Real-Time Monitoring

**CrewAI provides**:
- Agent thinking process (verbose=True)
- Task completion status
- Outputs generated

**Additional Monitoring**:

**Create**: `crews/monitor.py`

```python
import os
import time
from pathlib import Path

def monitor_progress():
    """Monitor task completion across all teams"""
    teams = ['team1_core', 'team2_security', 'team3_infra', 'team4_ml', 'team5_research']
    
    while True:
        print("\n" + "=" * 60)
        print(f"Chrysalis Execution Monitor - {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        for team in teams:
            team_dir = Path(f"crews/{team}")
            if not team_dir.exists():
                continue
            
            # Check for completed tasks (output files)
            completed = len(list(team_dir.glob("outputs/*.md")))
            total = get_total_tasks(team)
            
            print(f"{team}: {completed}/{total} tasks complete ({completed/total*100:.0f}%)")
        
        time.sleep(60)  # Update every minute

def get_total_tasks(team):
    # Read from YAML or crew.py
    # Return total task count
    pass

if __name__ == "__main__":
    monitor_progress()
```

**Run in separate terminal**:
```bash
python crews/monitor.py
```

### Progress Tracking

**GitHub Project Board**:
1. Create project in GitHub
2. Add columns: Backlog, In Progress, Review, Done
3. Create issues for each task
4. Agents update issues as they work

**Spreadsheet Tracking**:
- Track completion % per team
- Track hours spent vs estimated
- Track blockers and risks

---

## Handling Dependencies

### Phase 1 â†’ Phase 2 Handoff

**End of Week 6**:

1. **Verify Team 1 Deliverables**:
```bash
# Check MemoryMerger exists and is tested
test -f src/experience/MemoryMerger.ts && echo "âœ“ MemoryMerger exists"
test -f tests/experience/MemoryMerger.test.ts && echo "âœ“ Tests exist"

# Run tests
npm test -- MemoryMerger.test.ts

# Check coverage
npm run coverage | grep MemoryMerger
# Should be >90%
```

2. **Create Handoff Package**:
```bash
# Bundle artifacts for Team 4
mkdir -p handoffs/phase1_to_phase2/
cp src/experience/MemoryMerger.ts handoffs/phase1_to_phase2/
cp design/UniformSemanticAgentV2-Schema.md handoffs/phase1_to_phase2/
cp docs/technical/MemoryMerger.md handoffs/phase1_to_phase2/

# Create handoff document
cat > handoffs/phase1_to_phase2/README.md << 'EOF'
# Phase 1 â†’ Phase 2 Handoff

## Artifacts
- MemoryMerger.ts - Base implementation with Jaccard similarity
- Schema documentation
- API documentation

## Integration Points
- MemoryMergerConfig interface: Add similarity_method, embedding_service
- calculateSimilarity() method: Dispatch based on config

## Quality Status
- Test coverage: [X]%
- Performance: [X]ms for 100 memories
- Known limitations: O(NÂ²), <1000 memories

## Next Steps for Team 4
- Integrate EmbeddingService
- Add embeddingSimilarity() method
- Benchmark improvement
EOF
```

3. **Gate Review**:
```bash
# Run Phase 1 quality gate checks
./scripts/phase1-gate-check.sh

# If >90% pass, proceed to Phase 2
# If <90%, extend Phase 1
```

4. **Kickoff Team 4**:
```bash
cd crews/team4_ml
python crew.py  # Team 4 begins
```

---

## Managing CrewAI Agent Work

### Task Context Injection

**Problem**: Agents need access to specification documents

**Solution**: Inject file contents as context

```python
# In crew.py
def read_context_file(path):
    with open(path) as f:
        return f.read()

# Add to task
task = Task(
    description=f"""Design PatternResolver...
    
    CONTEXT FROM SPECIFICATION:
    {read_context_file('../../CHRYSALIS_UNIFIED_SPEC_V3.1.md')}
    
    CONTEXT FROM REVIEW:
    {read_context_file('../../SYNTHESIS_REPORT_FINAL.md')}
    """,
    agent=architect,
    # ...
)
```

### Tool Configuration

**File Operations**:
```python
from crewai_tools import FileReadTool, FileWriteTool

read_tool = FileReadTool(base_path='../../src')
write_tool = FileWriteTool(base_path='../../src')

agent = Agent(
    # ...
    tools=[read_tool, write_tool]
)
```

**Code Execution**:
```python
from crewai_tools import CodeInterpreterTool

code_tool = CodeInterpreterTool()

agent = Agent(
    # ...
    tools=[code_tool]
)
```

**Custom Tools**:
```python
from crewai_tools import BaseTool

class MermaidDiagramGenerator(BaseTool):
    name: str = "Mermaid Diagram Generator"
    description: str = "Generate Mermaid diagrams from descriptions"
    
    def _run(self, description: str) -> str:
        # Implementation
        return generated_diagram

# Use in agent
agent = Agent(
    # ...
    tools=[MermaidDiagramGenerator()]
)
```

### Output Management

**Structured Outputs**:
```python
task = Task(
    description="...",
    expected_output="...",
    output_file="design/PatternResolver-Design.md",  # Saves to file
    output_json=None,  # Or specify JSON schema
    output_pydantic=None,  # Or Pydantic model
    agent=agent
)
```

**Validation**:
```python
def validate_task_output(task, output):
    """Validate task output meets acceptance criteria"""
    if task.id == "TASK-1.1.1":
        # Check for required sections
        assert "Interface design" in output
        assert "Mermaid" in output
        # ... validate acceptance criteria
    return True
```

---

## Team Coordination

### Shared Workspace

**Create shared directory**:
```bash
mkdir -p workspace/shared
mkdir -p workspace/team1
mkdir -p workspace/team2
# ... etc
```

**Teams read from shared**:
```python
# Team 4 reads Team 1 output
memory_merger_path = "../workspace/team1/MemoryMerger.ts"
```

### Handoff Synchronization

**Option A: Blocking (Wait for File)**:
```python
import time
from pathlib import Path

def wait_for_handoff(file_path, timeout=3600):
    """Wait for Team 1 to produce artifact"""
    start = time.time()
    while time.time() - start < timeout:
        if Path(file_path).exists():
            return True
        time.sleep(60)  # Check every minute
    raise TimeoutError(f"Handoff artifact not ready: {file_path}")

# In Team 4 crew
wait_for_handoff("../workspace/team1/MemoryMerger.ts")
print("âœ“ Handoff received, Team 4 starting...")
```

**Option B: Manual (Human Coordination)**:
```bash
# After Team 1 completes
echo "âœ… Team 1 complete. Ready for Team 4."

# Human verifies, then starts Team 4
cd crews/team4_ml && python crew.py
```

---

## Quality Control

### Automated Validation

**Create**: `scripts/validate-deliverable.sh`

```bash
#!/bin/bash
# Validate that task deliverable meets acceptance criteria

TASK_ID=$1
DELIVERABLE=$2

case $TASK_ID in
  "TASK-1.2.1")
    # Validate PatternResolver
    echo "Checking PatternResolver.ts..."
    test -f "$DELIVERABLE" || exit 1
    grep -q "class AdaptivePatternResolver" "$DELIVERABLE" || exit 1
    grep -q "resolveHash" "$DELIVERABLE" || exit 1
    grep -q "resolveSignature" "$DELIVERABLE" || exit 1
    echo "âœ“ PatternResolver validation passed"
    ;;
  
  "TASK-1.2.2")
    # Validate Adapters
    echo "Checking adapters..."
    test -f "src/adapters/MCPAdapter.ts" || exit 1
    test -f "src/adapters/MultiAgentAdapter.ts" || exit 1
    test -f "src/adapters/OrchestratedAdapter.ts" || exit 1
    
    # Run lossless tests
    npm test -- adapters/
    ;;
  
  # ... more validation logic
esac

echo "âœ… Task $TASK_ID validated"
```

**Use after each task**:
```bash
./scripts/validate-deliverable.sh TASK-1.2.1 src/fabric/PatternResolver.ts
```

### Manual Review

**Review Checklist** (After each task):
```markdown
## Task Review: [TASK-ID]

### Deliverables
- [ ] All files created as specified
- [ ] File sizes reasonable (not empty, not huge)
- [ ] Code compiles (TypeScript)
- [ ] Tests exist and pass

### Quality
- [ ] Follows coding standards (ESLint)
- [ ] Properly formatted (Prettier)
- [ ] JSDoc comments present
- [ ] No TODOs or FIXMEs

### Functionality
- [ ] Acceptance criteria met (all items)
- [ ] Integration points work
- [ ] Performance targets met (if applicable)

### Documentation
- [ ] Technical docs complete
- [ ] Examples provided
- [ ] Known limitations documented

**Verdict**: âœ… Approve | âš ï¸ Minor issues | âŒ Reject

**Notes**: [Reviewer comments]
```

---

## Troubleshooting

### Issue: Agent Gets Stuck

**Symptoms**: Agent repeats same action, doesn't progress

**Solutions**:
1. Check token limit (GPT-4: 8K context, may need compression)
2. Simplify task description (too complex)
3. Provide more explicit context
4. Add intermediate checkpoints

```python
# Add checkpoints
task_complex = Task(
    description="Complex task...",
    expected_output="...",
    agent=agent,
    checkpoint=True  # Agent can ask for guidance
)
```

### Issue: Output Quality Low

**Symptoms**: Agent produces incomplete or incorrect code

**Solutions**:
1. Enhance agent backstory (more expertise)
2. Provide better examples in context
3. Add validation steps
4. Use better LLM (GPT-4 vs GPT-3.5)

```python
# Enhanced backstory
agent = Agent(
    role="...",
    backstory="""[Previous backstory]
    
    IMPORTANT: Always follow these principles:
    - Write production-grade code
    - Include comprehensive error handling
    - Add JSDoc comments for all public functions
    - Follow TypeScript strict mode
    - Write tests first (TDD)
    """,
    # ...
)
```

### Issue: Dependencies Not Met

**Symptoms**: Team 4 starts before Team 1 finishes

**Solutions**:
1. Check handoff artifacts exist
2. Use wait_for_handoff() function
3. Manual gate review before starting next phase

```python
# Enforce dependency
def check_dependency(team, task_id):
    if team == 'team4':
        # Check Team 1 completion
        required_files = [
            'src/experience/MemoryMerger.ts',
            'tests/experience/MemoryMerger.test.ts'
        ]
        for f in required_files:
            if not Path(f).exists():
                raise RuntimeError(f"Dependency not met: {f} missing")
```

---

## Execution Timeline

### Week-by-Week Execution Guide

**Week 1**:
```bash
# Start Phase 1 teams
python crews/orchestrator.py --phase 1 --week 1

# Monitor
python crews/monitor.py

# Expected: Architecture and design tasks complete
```

**Week 2-5**:
```bash
# Continue Phase 1
# No action needed if orchestrator is running

# Weekly check
python crews/status-report.py --week N
```

**Week 6**:
```bash
# Phase 1 completion
python crews/phase1-gate-check.py

# If pass:
echo "âœ… Phase 1 complete, ready for Phase 2"

# If fail:
echo "âš ï¸ Extend Phase 1 by 1-2 weeks"
```

**Week 7-12**:
```bash
# Start Phase 2 (Team 4)
python crews/orchestrator.py --phase 2
```

**Week 10-24**:
```bash
# Start Phase 3 (Team 5, can overlap with Phase 2)
python crews/orchestrator.py --phase 3
```

---

## Cost Management

### Estimate LLM Costs

**GPT-4 Pricing** (as of 2024):
- Input: $0.03/1K tokens
- Output: $0.06/1K tokens

**Estimated Token Usage**:
- Per task: ~50K tokens input (context) + ~10K tokens output = 60K tokens
- Per task cost: ~$2.10
- Total tasks: ~39 tasks
- **Total estimated**: ~$82 in LLM costs

**For GPT-4 Turbo** (~10x cheaper): ~$8

**For local LLM** (LM Studio, Ollama): $0 (just compute)

### Optimization

**Reduce costs**:
1. Use GPT-4 Turbo or GPT-3.5 for simple tasks
2. Cache responses (CrewAI supports caching)
3. Use local LLMs for non-critical tasks
4. Compress context (summarize specs)

```python
# Configure per agent
expensive_agent = Agent(
    # ...
    llm="gpt-4"  # For critical tasks
)

cheap_agent = Agent(
    # ...
    llm="gpt-3.5-turbo"  # For simpler tasks
)
```

---

## Output Organization

### Recommended Structure

```
Chrysalis/
â”œâ”€â”€ crews/                      # CrewAI agent crews
â”‚   â”œâ”€â”€ team1_core/
â”‚   â”‚   â”œâ”€â”€ crew.py
â”‚   â”‚   â”œâ”€â”€ outputs/            # Agent-generated outputs
â”‚   â”‚   â””â”€â”€ logs/               # Execution logs
â”‚   â”œâ”€â”€ team2_security/
â”‚   â”œâ”€â”€ team3_infra/
â”‚   â”œâ”€â”€ team4_ml/
â”‚   â”œâ”€â”€ team5_research/
â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â”œâ”€â”€ tools/              # Custom tools
â”‚   â”‚   â”œâ”€â”€ utils/              # Utilities
â”‚   â”‚   â””â”€â”€ templates/          # Templates
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ monitor.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ workspace/                  # Shared workspace
â”‚   â”œâ”€â”€ team1/                  # Team 1 outputs
â”‚   â”œâ”€â”€ team2/                  # Team 2 outputs
â”‚   â”œâ”€â”€ handoffs/               # Phase transition artifacts
â”‚   â””â”€â”€ shared/                 # Shared resources
â”‚
â”œâ”€â”€ src/                        # Source code (agent-generated)
â”œâ”€â”€ tests/                      # Tests (agent-generated)
â”œâ”€â”€ docs/                       # Documentation (agent-generated)
â”œâ”€â”€ design/                     # Design docs (agent-generated)
â””â”€â”€ ... (rest of Chrysalis structure)
```

---

## Best Practices

### 1. Incremental Execution

**Don't**: Run all 26 weeks at once  
**Do**: Run week by week, validate progress

```bash
# Week 1
python crews/orchestrator.py --phase 1 --weeks 1

# Review outputs
ls workspace/team1/week1/

# If good, continue
python crews/orchestrator.py --phase 1 --weeks 2

# Repeat...
```

### 2. Human-in-the-Loop

**Don't**: Fully automate without oversight  
**Do**: Review critical deliverables

**Critical Review Points**:
- End of Week 2 (architecture designs)
- End of Week 4 (core implementations)
- End of Week 6 (Phase 1 gate)
- End of Week 12 (Phase 2 gate)

### 3. Feedback Loops

**Don't**: Let agents run blindly  
**Do**: Provide feedback on outputs

```python
# In orchestrator
result = team.execute_task(task)
human_feedback = input("Review output. Feedback (or ENTER to accept): ")
if human_feedback:
    # Re-run task with feedback
    task.description += f"\n\nHUMAN FEEDBACK: {human_feedback}"
    result = team.execute_task(task)
```

### 4. Checkpoint & Resume

**Don't**: Lose progress on failure  
**Do**: Save state regularly

```python
import pickle

# Save crew state
with open('crew_checkpoint.pkl', 'wb') as f:
    pickle.dump(crew.state, f)

# Resume from checkpoint
with open('crew_checkpoint.pkl', 'rb') as f:
    crew.state = pickle.load(f)
    crew.resume()
```

---

## Execution Checklist

### Before Starting
- [ ] CrewAI installed and tested
- [ ] LLM configured (OpenAI key or local LLM)
- [ ] Repository cloned
- [ ] Crew directories created
- [ ] All 5 crew.py files created from YAML specs
- [ ] Orchestrator script created
- [ ] Monitoring script created

### Week 1 Startup
- [ ] Start Phase 1 teams (1, 2, 3)
- [ ] Verify agents are working (check logs)
- [ ] Monitor progress (crews/monitor.py)
- [ ] Review first outputs

### Week 6 Gate
- [ ] Run phase1-gate-check.sh
- [ ] Review all Team 1 deliverables
- [ ] Validate test coverage >85%
- [ ] Check security scan passed
- [ ] Decision: Go/No-Go for Phase 2

### Week 12 Gate
- [ ] Run phase2-gate-check.sh
- [ ] Verify memory system scales to 100K
- [ ] Validate O(log N) search empirically
- [ ] Check benchmarks complete

### Week 24 Completion
- [ ] All phases complete
- [ ] Final demo recorded
- [ ] Documentation published
- [ ] Papers submitted
- [ ] Community announcement

---

## Success Indicators

**Green Signals** (Execution going well):
- âœ… Tasks completing on schedule
- âœ… Outputs meet acceptance criteria
- âœ… Tests passing
- âœ… No major blockers
- âœ… Team velocity consistent

**Yellow Signals** (Watch closely):
- âš ï¸ Tasks taking 20% longer than estimated
- âš ï¸ Test failures increasing
- âš ï¸ Blockers emerge
- âš ï¸ Quality issues in reviews

**Red Signals** (Intervention needed):
- ðŸš¨ Tasks taking 50%+ longer
- ðŸš¨ Critical tests failing
- ðŸš¨ Teams blocked >3 days
- ðŸš¨ Major rework needed

---

## Example: Running Team 1 (Minimal Version)

```bash
# 1. Create simplified crew
cat > crews/team1_core/crew_minimal.py << 'EOF'
from crewai import Agent, Task, Crew, Process

architect = Agent(
    role="Platform Architect",
    goal="Design Chrysalis core platform",
    backstory="Senior architect with distributed systems expertise",
    verbose=True
)

engineer = Agent(
    role="Backend Engineer",
    goal="Implement Chrysalis core modules",
    backstory="Expert TypeScript engineer",
    verbose=True
)

task1 = Task(
    description="Design PatternResolver architecture with interfaces and diagrams",
    agent=architect,
    expected_output="Design document with interfaces and diagrams"
)

task2 = Task(
    description="Implement PatternResolver in TypeScript",
    agent=engineer,
    expected_output="src/fabric/PatternResolver.ts with tests",
    context=[task1]
)

crew = Crew(
    agents=[architect, engineer],
    tasks=[task1, task2],
    process=Process.SEQUENTIAL,
    verbose=True
)

if __name__ == "__main__":
    result = crew.kickoff()
    print(result)
EOF

# 2. Run it
cd crews/team1_core
python crew_minimal.py

# 3. Review outputs
cat outputs/task-1-output.md
cat outputs/task-2-output.md
```

---

## Conclusion

**Setup Complexity**: Medium (requires CrewAI familiarity)  
**Execution Complexity**: High (26 weeks, 5 teams, coordination)  
**Benefit**: Automated implementation from specifications  

**Recommendation**:
1. **Start small**: Run Team 1 minimal version first (2-3 tasks)
2. **Validate approach**: Does CrewAI produce good outputs?
3. **Scale up**: If successful, expand to full team plans
4. **Iterate**: Improve prompts and context based on results

**Alternative**: If CrewAI automation seems too complex, use plans as **human team roadmap** instead. The task decomposition, dependencies, and specifications are valuable regardless of automation.

---

ðŸ¦‹ **Plans are ready. Execution approach is clear. Begin when ready.** ðŸ¦‹

**Next**: Create crew.py files, test with minimal version, scale to full execution.
