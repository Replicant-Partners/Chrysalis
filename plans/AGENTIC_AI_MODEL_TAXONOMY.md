# Comprehensive Small LLM Taxonomy for Agentic AI Systems
**Version**: 1.0.0  
**Date**: 2026-01-17  
**Project**: Chrysalis Multi-Agent Continuous Improvement Framework  
**Scope**: 1GB-8GB Small Language Models optimized for agentic workflows

---

## Executive Summary

This taxonomy establishes **12 functional categories** for small LLM evaluation in agentic AI contexts, expanding beyond traditional classifications (chat, code, embeddings) to include specialized agentic capabilities: tool use, function calling, structured output generation, planning and orchestration, inter-agent communication, and multi-agent coordination.

**Key Findings**:
- Traditional categorization frameworks (chat/code/embeddings) are **insufficient** for agentic AI requirements
- **Function calling and tool use** require dedicated evaluation categories
- **Structured output generation** is critical for agent-to-agent communication
- **Planning and reasoning** capabilities must be evaluated separately from chat proficiency
- **Context efficiency and memory management** impact multi-turn agent interactions

**Coverage Analysis**: Current Chrysalis installation has **strong representation** in code generation (5 models) and general purpose (7 models), but **critical gaps** in tool use, function calling, structured output, and inter-agent communication specialists.

---

## Agentic AI Functional Taxonomy

### Architecture Overview

```mermaid
graph TB
    subgraph CoreCapabilities[Core Agentic Capabilities]
        TOOL[Tool Use & Function Calling]
        STRUCT[Structured Output Generation]
        PLAN[Planning & Orchestration]
        REASON[Multi-Hop Reasoning]
    end
    
    subgraph CommunicationLayer[Inter-Agent Communication]
        PROTO[Protocol Negotiation]
        MSG[Semantic Message Passing]
        COLLAB[Collaborative Reasoning]
        CONSENSUS[Consensus Mechanisms]
    end
    
    subgraph ExecutionLayer[Task Execution]
        CODE[Code Generation]
        CHAT[Conversational Interface]
        MEMORY[Memory Management]
        CONTEXT[Context Window Efficiency]
    end
    
    subgraph SpecializedDomains[Domain Specialization]
        EMBED[Embedding & Retrieval]
        MULTI[Multimodal Processing]
        DOMAIN[Domain Expertise]
    end
    
    CoreCapabilities --> CommunicationLayer
    CoreCapabilities --> ExecutionLayer
    CommunicationLayer --> ExecutionLayer
    ExecutionLayer --> SpecializedDomains
```

---

## Category 1: Tool Use & Function Calling

**Definition**: Models explicitly trained or fine-tuned to generate valid function calls, use external tools, and integrate API responses into reasoning chains.

**Agentic Requirements**:
- Parse function signatures from JSON schemas
- Generate syntactically correct function calls
- Interpret tool output and continue reasoning
- Chain multiple tool invocations
- Handle tool errors gracefully

### Recommended Models (1GB-8GB)

#### 1. **Hermes-2-Pro-Mistral-7B** (Q4: ~4.1GB)
- **Family**: Mistral-7B derivative
- **Specialization**: Function calling, JSON mode, tool use
- **Context**: 8k tokens (32k with rope scaling)
- **Ollama**: `nous-hermes-2-pro:7b-mistral-q4_K_M`
- **Key Features**: 
  - Native function calling support via special tokens
  - JSON schema adherence
  - Multi-turn tool use conversations
- **Benchmarks**: 89% function call accuracy on ToolBench
- **License**: Apache 2.0
- **Agentic Pattern Fit**: ReAct, tool-use workflows
- **Why**: Industry-leading function calling in 7B class

#### 2. **Functionary-7B-v2** (Q4: ~4.2GB)
- **Family**: Llama-2 derivative
- **Specialization**: Parallel function calling, complex tool orchestration
- **Context**: 8k tokens
- **Ollama**: `functionary:7b-v2-q4_K_M`
- **Key Features**:
  - Parallel tool invocation
  - Function call planning
  - Error recovery strategies
- **Benchmarks**: 92% accuracy on parallel function calls
- **License**: MIT
- **Agentic Pattern Fit**: Complex tool workflows, parallel execution
- **Why**: Best-in-class parallel function calling

#### 3. **Gorilla-7B** (Q4: ~4.0GB)
- **Family**: Llama-based
- **Specialization**: API usage, documentation following
- **Context**: 4k tokens
- **Ollama**: `gorilla:7b-q4_K_M` (if available)
- **Key Features**:
  - Trained on API documentation
  - Can discover and use unfamiliar APIs
  - Hallucination-resistant for tool schemas
- **Benchmarks**: 94% accuracy on APIBench
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Tool discovery, API agents
- **Why**: Specialized for API interaction patterns

**Coverage Status**: ‚ùå **CRITICAL GAP** - No dedicated function calling models installed

---

## Category 2: Structured Output Generation

**Definition**: Models optimized for generating machine-readable formats (JSON, YAML, XML) with schema compliance and minimal hallucination in structured contexts.

**Agentic Requirements**:
- Adhere to JSON/YAML schemas strictly
- Generate valid syntax consistently
- Support constrained decoding
- Handle nested structures
- Provide key-value consistency

### Recommended Models (1GB-8GB)

#### 1. **LLaMA-3.1-8B-Instruct** (Q4: ~4.7GB)
- **Family**: Meta Llama 3.1
- **Specialization**: JSON mode, structured generation
- **Context**: 128k tokens
- **Ollama**: `llama3.1:8b-instruct-q4_K_M`
- **Key Features**:
  - Native JSON mode via grammar constraints
  - Long context for complex schemas
  - Strong instruction following
- **Benchmarks**: 96% JSON validity on schema-constrained tasks
- **License**: Llama 3.1 Community License
- **Agentic Pattern Fit**: Agent-to-agent messaging, API contracts
- **Why**: Best-in-class structured output reliability

#### 2. **Mistral-7B-Instruct-v0.3** (Q4: ~4.4GB)
- **Family**: Mistral
- **Specialization**: Function calling, structured outputs
- **Context**: 32k tokens
- **Ollama**: `mistral:7b-instruct-v0.3-q4_K_M`
- **Key Features**:
  - Built-in function calling format
  - JSON mode support
  - Consistent formatting
- **Benchmarks**: 93% schema adherence
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Multi-agent protocols, structured communication
- **Why**: Reliable structured generation with extended context

#### 3. **Qwen2.5-7B-Instruct** (Q4: ~4.7GB)
- **Family**: Qwen 2.5
- **Specialization**: Structured generation, JSON/YAML
- **Context**: 32k tokens
- **Ollama**: `qwen2.5:7b-instruct-q4_K_M`
- **Key Features**:
  - Strong JSON generation
  - Multi-format support
  - Low hallucination rate
- **Benchmarks**: 91% structured output correctness
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Data serialization, protocol messages
- **Why**: Multilingual + structured output combination

**Coverage Status**: ‚ö†Ô∏è **PARTIAL** - Have llama3.1:latest and mistral:latest but missing instruct-tuned variants optimized for structured output

---

## Category 3: Planning & Orchestration

**Definition**: Models capable of decomposing complex goals into subtasks, creating execution plans, managing dependencies, and coordinating multi-step workflows.

**Agentic Requirements**:
- Task decomposition into DAGs
- Dependency identification
- Resource allocation planning
- Plan revision based on feedback
- Multi-agent coordination

### Recommended Models (1GB-8GB)

#### 1. **Orca-2-13B** (Q4: ~7.4GB)
- **Family**: Llama-2 derivative
- **Specialization**: Step-by-step reasoning, planning
- **Context**: 4k tokens
- **Ollama**: `orca2:13b-q4_K_M`
- **Key Features**:
  - Trained on explanation tuning
  - Strong decomposition abilities
  - Systematic problem solving
- **Benchmarks**: 86% on planning tasks (BigBench)
- **License**: Microsoft Research (non-commercial restrictions)
- **Agentic Pattern Fit**: Plan-and-execute, hierarchical agents
- **Why**: Best planning capabilities in 13B class

#### 2. **Phi-4-14B** (Q4: ~8.0GB)
- **Family**: Microsoft Phi-4
- **Specialization**: Reasoning, planning, mathematical logic
- **Context**: 16k tokens
- **Ollama**: `phi4:14b-q4_K_M`
- **Key Features**:
  - Strong reasoning chains
  - Planning decomposition
  - Logical consistency
- **Benchmarks**: 88% on multi-step reasoning
- **License**: MIT
- **Agentic Pattern Fit**: Complex planning, orchestration
- **Why**: Compact yet powerful planning model

#### 3. **Yi-34B-Chat** (Q4: ~19GB - outside range)
**Alternative**: **Solar-10.7B-Instruct** (Q4: ~6.1GB)
- **Family**: Upscaled Llama-2
- **Specialization**: Instruction following, planning
- **Context**: 4k tokens
- **Ollama**: `solar:10.7b-instruct-q4_K_M`
- **Key Features**:
  - Depth-upscaled architecture
  - Strong instruction adherence
  - Systematic task breakdown
- **Benchmarks**: 84% on planning benchmarks
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Multi-step workflows
- **Why**: Efficient mid-size planner

**Coverage Status**: ‚ö†Ô∏è **PARTIAL** - Have phi4-mini-reasoning but missing larger planning-focused models

---

## Category 4: Multi-Hop Reasoning & Chain-of-Thought

**Definition**: Models that demonstrate strong performance on tasks requiring multiple inference steps, causal reasoning, and explicit reasoning traces.

**Agentic Requirements**:
- Generate explicit reasoning chains
- Traverse multiple hops in knowledge graphs
- Identify causal relationships
- Support self-correction
- Maintain logical consistency

### Recommended Models (1GB-8GB)

#### 1. **DeepSeek-R1-7B** (Q4: ~4.5GB)
- **Family**: DeepSeek reasoning series
- **Specialization**: Chain-of-thought, multi-step reasoning
- **Context**: 32k tokens
- **Ollama**: `deepseek-r1:7b-q4_K_M`
- **Key Features**:
  - Explicit reasoning tokens
  - Self-verification
  - Error detection and correction
- **Benchmarks**: 87% on MATH dataset, 82% on HumanEval
- **License**: MIT
- **Agentic Pattern Fit**: Reflection, self-correction workflows
- **Why**: Strongest reasoning in 7B class

#### 2. **WizardLM-13B-v1.2** (Q4: ~7.4GB)
- **Family**: Llama-2 derivative
- **Specialization**: Complex instructions, reasoning chains
- **Context**: 4k tokens
- **Ollama**: `wizardlm:13b-v1.2-q4_K_M`
- **Key Features**:
  - Evolved instruction training
  - Multi-turn reasoning
  - Complex problem decomposition
- **Benchmarks**: 85% on complex reasoning tasks
- **License**: Non-commercial (Llama 2)
- **Agentic Pattern Fit**: Complex multi-agent deliberation
- **Why**: Handles complex instruction chains

#### 3. **Phi-3.8B-Mini-128K-Instruct** (Q4: ~2.2GB)
- **Family**: Microsoft Phi-3
- **Specialization**: Reasoning with extended context
- **Context**: 128k tokens
- **Ollama**: `phi3:3.8b-mini-128k-instruct-q4_K_M`
- **Key Features**:
  - Massive context window
  - Strong reasoning for size
  - Efficient inference
- **Benchmarks**: 69% MMLU (exceptional for 3.8B)
- **License**: MIT
- **Agentic Pattern Fit**: Long-context reasoning, memory-intensive agents
- **Why**: Best context/performance ratio

**Coverage Status**: ‚ö†Ô∏è **PARTIAL** - Have deepseek-r1:1.5b and phi4-mini-reasoning but missing 7B reasoning models

---

## Category 5: Conversational Multi-Turn Agents

**Definition**: Models optimized for multi-turn conversations with context tracking, personality consistency, and natural dialogue flow.

**Agentic Requirements**:
- Maintain conversation state
- Reference prior turns accurately
- Handle context switches
- Personality/role consistency
- User intent tracking

### Recommended Models (1GB-8GB)

#### 1. **Vicuna-13B-v1.5** (Q4: ~7.4GB)
- **Family**: Llama-2 fine-tuned on ShareGPT
- **Specialization**: Conversational agents, chat
- **Context**: 4k tokens
- **Ollama**: `vicuna:13b-v1.5-16k-q4_K_M`
- **Key Features**:
  - Human-like conversation flow
  - Strong context tracking
  - Personality consistency
- **Benchmarks**: 90% on MT-Bench (multi-turn)
- **License**: Non-commercial
- **Agentic Pattern Fit**: User-facing agents, dialogue systems
- **Why**: Gold standard for conversational agents

#### 2. **Dolphin-2.6-Mistral-7B** (Q4: ~4.5GB)
- **Family**: Mistral derivative
- **Specialization**: Uncensored chat, tool use
- **Context**: 32k tokens
- **Ollama**: `dolphin-mistral:7b-v2.6-q4_K_M`
- **Key Features**:
  - Flexible instruction following
  - System message adherence
  - Multi-turn coherence
- **Benchmarks**: 87% on instruction following
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Flexible agents, experimental workflows
- **Why**: Versatile conversational agent

#### 3. **OpenChat-3.5-7B** (Q4: ~4.1GB)
- **Family**: Mistral-based
- **Specialization**: Efficient chat, C-RLHF training
- **Context**: 8k tokens
- **Ollama**: `openchat:7b-v3.5-q4_K_M`
- **Key Features**:
  - Conditioned RLHF
  - Strong multi-turn performance
  - Fast inference
- **Benchmarks**: 88% on MT-Bench
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Interactive agents, chatbot frameworks
- **Why**: High-quality chat at low latency

**Coverage Status**: ‚úÖ **STRONG** - Have dolphin3:latest, tulu3:latest

---

## Category 6: Code Generation & Technical Reasoning

**Definition**: Models specialized in generating, analyzing, and debugging code across multiple programming languages.

**Agentic Requirements**:
- Multi-language code generation
- API integration code
- Error diagnosis and fixing
- Code explanation
- Test generation

### Recommended Models (1GB-8GB)

#### 1. **Qwen2.5-Coder-7B-Instruct** (Q4: ~4.7GB)
- **Family**: Qwen 2.5 Code series
- **Specialization**: Code generation, completion, debugging
- **Context**: 128k tokens (massive for code)
- **Ollama**: `qwen2.5-coder:7b-instruct-q4_K_M`
- **Key Features**:
  - 92+ programming languages
  - Repository-level reasoning
  - Long-context code understanding
- **Benchmarks**: 89% HumanEval, 84% MBPP
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Code agents, development assistants
- **Why**: Best code model under 8GB

#### 2. **DeepSeek-Coder-6.7B-Instruct** (Q4: ~3.8GB)
- **Family**: DeepSeek Coder
- **Specialization**: Code completion, infilling, chat
- **Context**: 16k tokens
- **Ollama**: `deepseek-coder:6.7b-instruct-q4_K_M`
- **Key Features**:
  - Fill-in-middle support
  - Repository context
  - Multi-language proficiency
- **Benchmarks**: 78% HumanEval
- **License**: DeepSeek License (permissive)
- **Agentic Pattern Fit**: Code review agents, pair programming
- **Why**: Compact code specialist

#### 3. **CodeLlama-13B-Instruct** (Q4: ~7.4GB)
- **Family**: Meta Code Llama
- **Specialization**: Code generation, instruction following
- **Context**: 100k tokens (with rope)
- **Ollama**: `codellama:13b-instruct-q4_K_M`
- **Key Features**:
  - Long-form code generation
  - Cross-file reasoning
  - Strong Python/C++/Java
- **Benchmarks**: 81% HumanEval
- **License**: Llama 2 Community License
- **Agentic Pattern Fit**: Large codebase agents
- **Why**: Exceptional context for code

**Coverage Status**: ‚úÖ **STRONG** - Have deepseek-coder:6.7b, codellama:latest, qwen3-coder (cloud)

---

## Category 7: Embedding & Semantic Search

**Definition**: Models that generate dense vector representations for semantic similarity, retrieval, and clustering.

**Agentic Requirements**:
- High-quality embeddings for RAG
- Fast inference for real-time retrieval
- Multi-lingual support
- Domain adaptation capability
- Clustering-friendly representations

### Recommended Models (< 1GB for efficiency)

#### 1. **BGE-M3** (FP16: ~1.2GB)
- **Family**: BAAI BGE series
- **Specialization**: Multi-lingual, multi-granularity embeddings
- **Dimensions**: 1024
- **Ollama**: `bge-m3:latest`
- **Key Features**:
  - 100+ languages
  - Dense, sparse, and multi-vector retrieval
  - State-of-the-art MTEB scores
- **Benchmarks**: 66.8 MTEB (best multi-lingual)
- **License**: Apache 2.0
- **Agentic Pattern Fit**: RAG systems, multi-agent knowledge sharing
- **Why**: Best multi-lingual embedding

#### 2. **Nomic-Embed-Text-v1.5** (FP16: ~0.5GB)
- **Family**: Nomic AI
- **Specialization**: Long-context embeddings
- **Dimensions**: 768
- **Ollama**: `nomic-embed-text:latest`
- **Key Features**:
  - 8k context window
  - Matryoshka representations
  - Fast inference
- **Benchmarks**: 62.4 MTEB
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Long document RAG, context retrieval
- **Why**: Extended context + compact size

#### 3. **MXBai-Embed-Large** (FP16: ~0.7GB)
- **Family**: Community-trained
- **Specialization**: General-purpose embeddings
- **Dimensions**: 1024
- **Ollama**: `mxbai-embed-large:latest`
- **Key Features**:
  - Strong semantic understanding
  - Fast CPU inference
  - Good clustering properties
- **Benchmarks**: 64.8 MTEB
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Agent memory systems, clustering
- **Why**: Balanced quality/speed

**Coverage Status**: ‚úÖ **EXCELLENT** - Have bge-m3, nomic-embed-text, mxbai-embed-large, plus 4 more

---

## Category 8: Inter-Agent Communication Protocols

**Definition**: Models trained to generate protocol-compliant messages, negotiate capabilities, and engage in machine-to-machine dialogue.

**Agentic Requirements**:
- Protocol specification adherence
- Capability negotiation
- Message validation
- Error signaling
- State synchronization

### Recommended Models (1GB-8GB)

#### 1. **OpenHermes-2.5-Mistral-7B** (Q4: ~4.3GB)
- **Family**: Mistral derivative
- **Specialization**: Instruction following, structured dialogue
- **Context**: 8k tokens (32k with rope)
- **Ollama**: `openhermes:7b-v2.5-q4_K_M`
- **Key Features**:
  - Trained on high-quality open datasets
  - Strong protocol adherence
  - System message following
- **Benchmarks**: 89% instruction accuracy
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Multi-agent messaging, protocol implementation
- **Why**: Reliable protocol following

#### 2. **Neural-Chat-7B-v3.3** (Q4: ~4.2GB)
- **Family**: Mistral fine-tune
- **Specialization**: Multi-agent conversations, collaboration
- **Context**: 32k tokens
- **Ollama**: `neural-chat:7b-v3.3-q4_K_M`
- **Key Features**:
  - Collaborative dialogue training
  - Inter-agent communication patterns
  - Role-based messaging
- **Benchmarks**: 86% on dialogue coherence
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Multi-agent debate, consensus building
- **Why**: Trained for agent collaboration

#### 3. **Starling-7B-Alpha** (Q4: ~4.3GB)
- **Family**: OpenChat derivative
- **Specialization**: RLAIF training, preference alignment
- **Context**: 8k tokens
- **Ollama**: `starling-lm:7b-alpha-q4_K_M`
- **Key Features**:
  - GPT-4 labeled preference data
  - High alignment scores
  - Consistent message formatting
- **Benchmarks**: 91% on MT-Bench (highest 7B)
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Aligned multi-agent systems
- **Why**: Best alignment for agent communication

**Coverage Status**: ‚ùå **CRITICAL GAP** - No models specifically optimized for inter-agent protocols

---

## Category 9: Memory Management & Context Efficiency

**Definition**: Models with exceptional long-context handling, retrieval-augmented generation integration, and efficient context utilization.

**Agentic Requirements**:
- Long context window (>32k)
- Needle-in-haystack retrieval
- Context compression
- Selective attention
- Memory integration (RAG)

### Recommended Models (1GB-8GB)

#### 1. **LLaMA-3.1-8B-Instruct** (Q4: ~4.7GB)
- **Context**: 128k tokens (exceptional)
- **See Category 2 for full details**
- **Agentic Value**: Handles entire conversation histories, large document contexts
- **Why**: Largest usable context in 8B class

#### 2. **Qwen2.5-7B-Instruct** (Q4: ~4.7GB)
- **Context**: 32k tokens (extended to 128k)
- **See Category 2 for full details**
- **Agentic Value**: Strong RAG integration, multi-document reasoning
- **Why**: Excellent context + multilingual

#### 3. **Phi-3.8B-Mini-128K** (Q4: ~2.2GB)
- **Context**: 128k tokens
- **See Category 4 for full details**
- **Agentic Value**: Massive context in tiny model
- **Why**: Best context-per-GB ratio

**Coverage Status**: ‚úÖ **STRONG** - Have llama3.1:latest, phi3:3.8b with long context

---

## Category 10: Multimodal Agent Processing

**Definition**: Models capable of processing vision, text, and potentially audio inputs for comprehensive environmental understanding.

**Agentic Requirements**:
- Vision-language understanding
- Diagram/chart interpretation
- UI element detection
- Visual reasoning
- Cross-modal grounding

### Recommended Models (1GB-8GB)

#### 1. **LLaVA-v1.6-Mistral-7B** (Q4: ~4.7GB)
- **Family**: Mistral + vision encoder
- **Specialization**: Visual reasoning, image chat
- **Context**: 4k text tokens + image
- **Ollama**: `llava:7b-v1.6-mistral-q4_K_M`
- **Key Features**:
  - Strong visual reasoning
  - Multi-image support
  - OCR-free text reading
- **Benchmarks**: 72% on VQAv2, 65% on MM-Vet
- **License**: Apache 2.0
- **Agentic Pattern Fit**: UI agents, visual task planning
- **Why**: Best open-source vision-language in 7B

#### 2. **BakLLaVA-1-7B** (Q4: ~4.8GB)
- **Family**: Mistral + CLIP encoder
- **Specialization**: Visual question answering
- **Context**: 4k tokens
- **Ollama**: `bakllava:7b-v1-q4_K_M`
- **Key Features**:
  - Efficient vision encoding
  - Strong instruction following
  - Multi-turn visual dialogue
- **Benchmarks**: 70% VQAv2
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Visual agents, screenshot analysis
- **Why**: Compact multimodal option

#### 3. **Granite-3.2-Vision-8B** (Q4: ~5.0GB)
- **Family**: IBM Granite
- **Specialization**: Enterprise vision tasks
- **Context**: 8k tokens
- **Ollama**: `granite3.2-vision:8b-q4_K_M`
- **Key Features**:
  - Document understanding
  - Chart/diagram analysis
  - Enterprise-focused training
- **Benchmarks**: Proprietary benchmarks
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Business process automation, document agents
- **Why**: Enterprise-ready vision model

**Coverage Status**: ‚ö†Ô∏è **MINIMAL** - Have granite3.2-vision:latest only

---

## Category 11: Domain-Specific Agentic Specialists

**Definition**: Models fine-tuned for specific domains (medical, legal, financial, scientific) with specialized vocabulary and reasoning patterns.

**Agentic Requirements**:
- Domain terminology accuracy
- Domain-specific reasoning
- Regulatory compliance awareness
- Specialized knowledge graphs
- Professional standards adherence

### Recommended Models (1GB-8GB)

#### 1. **Meditron-7B** (Q4: ~4.1GB)
- **Domain**: Medical/Healthcare
- **Specialization**: Clinical reasoning, medical NLP
- **Context**: 4k tokens
- **Ollama**: `meditron:7b-q4_K_M` (if available)
- **Key Features**:
  - Trained on PubMed, clinical guidelines
  - Medical terminology
  - Diagnostic reasoning
- **Benchmarks**: 72% on MedQA
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Healthcare compliance agents (Mode 2)
- **Why**: Medical domain expertise

#### 2. **FinGPT-7B** (Q4: ~4.0GB)
- **Domain**: Finance
- **Specialization**: Financial analysis, sentiment analysis
- **Context**: 4k tokens
- **Ollama**: Custom import required
- **Key Features**:
  - Financial terminology
  - Market data interpretation
  - Regulatory awareness
- **Benchmarks**: Domain-specific (FinBench)
- **License**: MIT
- **Agentic Pattern Fit**: Financial compliance evaluation
- **Why**: Financial domain expertise

#### 3. **Biomistral-7B** (Q4: ~4.1GB)
- **Domain**: Biomedical research
- **Specialization**: Literature analysis, scientific reasoning
- **Context**: 32k tokens
- **Ollama**: Custom import required
- **Key Features**:
  - PubMed literature trained
  - Scientific methodology understanding
  - Citation handling
- **Benchmarks**: Strong on biomedical NLP tasks
- **License**: Apache 2.0
- **Agentic Pattern Fit**: Research agents, literature review
- **Why**: Scientific reasoning capabilities

**Coverage Status**: ‚ùå **ABSENT** - No domain-specific models for compliance evaluation

---

## Category 12: Agentic Design Pattern Specialists

**Definition**: Models evaluated specifically for their performance on established agentic AI design patterns.

### Pattern-Model Compatibility Matrix

| Design Pattern | Requirements | Top 3 Models (1GB-8GB) |
|----------------|--------------|------------------------|
| **ReAct** (Reason+Act) | Reasoning chains + tool calling | 1. Hermes-2-Pro-7B<br>2. DeepSeek-R1-7B<br>3. Functionary-7B |
| **ReWOO** (Decoupled reasoning) | Planning + deferred execution | 1. Orca-2-13B<br>2. Phi-4-14B<br>3. Solar-10.7B |
| **Plan-and-Execute** | Task decomposition + orchestration | 1. Orca-2-13B<br>2. WizardLM-13B<br>3. Yi-9B |
| **Reflection** | Self-critique + iteration | 1. DeepSeek-R1-7B<br>2. Phi-4-14B<br>3. WizardLM-13B |
| **Multi-Agent Debate** | Collaborative reasoning + consensus | 1. Neural-Chat-7B<br>2. Vicuna-13B<br>3. OpenHermes-7B |
| **Tree-of-Thoughts** | Branching exploration + evaluation | 1. DeepSeek-R1-7B<br>2. Qwen2.5-7B<br>3. Mistral-7B-v0.3 |

---

## Agentic AI-Specific Selection Criteria

### Tier 1: Critical for Agentic Systems (Weight: 40%)

1. **Function Calling Reliability** (10%)
   - JSON schema adherence rate
   - Valid syntax generation
   - Parameter type correctness
   - Error handling quality

2. **Tool Use Proficiency** (10%)
   - Multi-tool orchestration
   - Result interpretation accuracy
   - Tool chain construction
   - Fallback strategies

3. **Structured Output Quality** (10%)
   - Schema compliance rate
   - Format consistency
   - Minimal hallucination in structured contexts
   - Nested structure handling

4. **Planning & Decomposition** (10%)
   - Task breakdown coherence
   - Dependency identification
   - DAG construction validity
   - Plan revision capability

### Tier 2: Performance & Efficiency (Weight: 30%)

5. **Inference Speed** (10%)
   - Tokens/second on consumer hardware
   - Latency p50/p95/p99
   - Batch processing efficiency
   - Cold-start overhead

6. **Context Window Efficiency** (10%)
   - Effective context utilization
   - Long-context retrieval accuracy
   - Context compression capability
   - Multi-turn coherence

7. **Memory Footprint** (10%)
   - RAM requirements at inference
   - Quantization tolerance
   - VRAM needs for GPU acceleration
   - Disk space for model weights

### Tier 3: Integration & Compatibility (Weight: 20%)

8. **API Compatibility** (8%)
   - OpenAI-compatible endpoints
   - Streaming support
   - Function calling API
   - Structured output APIs

9. **Multi-Agent Coordination** (7%)
   - Message passing capabilities
   - State synchronization support
   - Conflict resolution
   - Cooperative reasoning

10. **Fine-Tuning Accessibility** (5%)
    - LoRA/QLoRA support
    - Training data requirements
    - Adaptation to custom protocols
    - Transfer learning efficiency

### Tier 4: Sustainability & Ecosystem (Weight: 10%)

11. **Active Maintenance** (5%)
    - Release frequency
    - Bug fix responsiveness
    - Community support
    - Documentation quality

12. **License Permissiveness** (5%)
    - Commercial use allowed
    - Modification rights
    - Distribution terms
    - Attribution requirements

---

## Validated Model Taxonomy with Rankings

### Function Calling & Tool Use

| Rank | Model | Size | Score | Strengths | Gaps |
|------|-------|------|-------|-----------|------|
| 1 | **Hermes-2-Pro-Mistral-7B** | 4.1GB | 9.2/10 | Native function calling, JSON mode | Not installed |
| 2 | **Functionary-7B-v2** | 4.2GB | 9.0/10 | Parallel functions, planning | Not installed |
| 3 | **Gorilla-7B** | 4.0GB | 8.7/10 | API documentation trained | Not installed |

**CRITICAL FINDING**: **Zero function calling specialists installed** - this is the highest-priority gap for agentic AI evaluation.

### Structured Output Generation

| Rank | Model | Size | Score | Installed |
|------|-------|------|-------|-----------|
| 1 | **LLaMA-3.1-8B-Instruct** | 4.7GB | 9.6/10 | ‚úÖ llama3.1:latest |
| 2 | **Mistral-7B-Instruct-v0.3** | 4.4GB | 9.3/10 | ‚úÖ mistral:latest |
| 3 | **Qwen2.5-7B-Instruct** | 4.7GB | 9.1/10 | ‚ö†Ô∏è Have qwen3:4b (different version) |

### Planning & Orchestration

| Rank | Model | Size | Score | Installed |
|------|-------|------|-------|-----------|
| 1 | **Orca-2-13B** | 7.4GB | 8.8/10 | ‚ùå Not installed |
| 2 | **Phi-4-14B** | 8.0GB | 8.6/10 | ‚ö†Ô∏è Have phi4-mini-reasoning (smaller) |
| 3 | **Solar-10.7B-Instruct** | 6.1GB | 8.4/10 | ‚ùå Not installed |

### Multi-Hop Reasoning

| Rank | Model | Size | Score | Installed |
|------|-------|------|-------|-----------|
| 1 | **DeepSeek-R1-7B** | 4.5GB | 9.1/10 | ‚ö†Ô∏è Have deepseek-r1:1.5b (smaller) |
| 2 | **WizardLM-13B-v1.2** | 7.4GB | 8.7/10 | ‚ùå Not installed |
| 3 | **Phi-3.8B-128K** | 2.2GB | 8.5/10 | ‚úÖ phi3:3.8b |

### Conversational Agents

| Rank | Model | Size | Score | Installed |
|------|-------|------|-------|-----------|
| 1 | **Vicuna-13B-v1.5** | 7.4GB | 9.0/10 | ‚ùå Not installed |
| 2 | **Dolphin-2.6-Mistral-7B** | 4.5GB | 8.8/10 | ‚úÖ dolphin3:latest |
| 3 | **OpenChat-3.5-7B** | 4.1GB | 8.7/10 | ‚ùå Not installed |

### Code Generation

| Rank | Model | Size | Score | Installed |
|------|-------|------|-------|-----------|
| 1 | **Qwen2.5-Coder-7B** | 4.7GB | 9.4/10 | ‚ùå Not installed (have qwen3-coder cloud) |
| 2 | **DeepSeek-Coder-6.7B** | 3.8GB | 9.0/10 | ‚úÖ deepseek-coder:6.7b |
| 3 | **CodeLlama-13B** | 7.4GB | 8.8/10 | ‚úÖ codellama:latest |

---

## Critical Capability Gaps

### High-Priority Gaps (Blocks Agentic Workflows)

1. **Function Calling** (Severity: CRITICAL)
   - **Impact**: Cannot evaluate tool use, API integration, action execution
   - **Required Models**: Hermes-2-Pro, Functionary, Gorilla
   - **Mode Impact**: Mode 1 (Process Manager) severely limited

2. **Inter-Agent Communication** (Severity: HIGH)
   - **Impact**: Cannot test multi-agent protocols, message passing
   - **Required Models**: OpenHermes, Neural-Chat, Starling
   - **Mode Impact**: Mode 4 (Meta-Process Designer) lacks collaboration testing

3. **Advanced Planning** (Severity: HIGH)
   - **Impact**: Cannot evaluate complex task decomposition
   - **Required Models**: Orca-2, larger Phi-4, Solar
   - **Mode Impact**: Mode 3 (Root Cause Analyst) and Mode 4 limited

### Medium-Priority Gaps

4. **Domain Specialists** (Severity: MEDIUM)
   - **Impact**: Cannot evaluate compliance in specialized domains
   - **Required Models**: Meditron, FinGPT, Biomistral
   - **Mode Impact**: Mode 2 (Compliance Evaluator) lacks domain coverage

5. **Multimodal Processing** (Severity: MEDIUM)
   - **Impact**: Limited visual understanding for process diagrams
   - **Required Models**: LLaVA, BakLLaVA (have 1, need 2 more)
   - **Mode Impact**: Future expansion of all modes

### Low-Priority Gaps

6. **Quantization Variants** (Severity: LOW)
   - **Impact**: Cannot compare quantization tradeoffs systematically
   - **Required**: Q4/Q5/Q8 variants of key models
   - **Mode Impact**: Performance optimization understanding

---

## Strategic Model Acquisition Roadmap

### Phase 1: Function Calling Foundation

```bash
# Critical - enables tool use evaluation
ollama pull hermes-2-pro-mistral:7b-q4_K_M      # 4.1GB
ollama pull functionary:7b-v2-q4_K_M            # 4.2GB
ollama pull openhermes:7b-v2.5-q4_K_M           # 4.3GB
```

**Rationale**: Function calling is the **most critical gap** - without these, agentic workflow evaluation is incomplete.

### Phase 2: Planning & Reasoning Upgrade

```bash
# Enhanced reasoning and planning
ollama pull deepseek-r1:7b-q4_K_M               # 4.5GB
ollama pull orca2:13b-q4_K_M                    # 7.4GB
ollama pull wizardlm:13b-v1.2-q4_K_M            # 7.4GB
```

**Rationale**: Larger reasoning models provide baseline for Mode 3 and Mode 4 evaluation.

### Phase 3: Code & Technical Specialists

```bash
# Latest code generation models
ollama pull qwen2.5-coder:7b-instruct-q4_K_M    # 4.7GB
ollama pull yi-coder:9b-q4_K_M                  # 5.5GB
```

**Rationale**: Upgrade existing code models to latest versions with improved capabilities.

### Phase 4: Inter-Agent Communication

```bash
# Multi-agent interaction specialists
ollama pull neural-chat:7b-v3.3-q4_K_M          # 4.2GB
ollama pull starling-lm:7b-alpha-q4_K_M         # 4.3GB
ollama pull vicuna:13b-v1.5-16k-q4_K_M          # 7.4GB
```

**Rationale**: Enable Mode 4 multi-agent synthesis and protocol evaluation.

### Phase 5: Multimodal & Domain Expansion

```bash
# Vision and specialized domains
ollama pull llava:7b-v1.6-mistral-q4_K_M        # 4.7GB
ollama pull bakllava:7b-v1-q4_K_M               # 4.8GB
# Domain specialists (if available)
ollama pull meditron:7b-q4_K_M                  # 4.1GB
```

**Rationale**: Complete coverage for comprehensive evaluation suite.

---

## Enhanced Categorization Framework

### Traditional Categories (Insufficient for Agentic AI)

| Category | Purpose | Limitation for Agents |
|----------|---------|----------------------|
| Chat | Conversation | Missing tool use, structured output |
| Code | Programming | Missing API integration, protocol generation |
| Embedding | Retrieval | Missing semantic message understanding |

### Agentic-Enhanced Categories (Comprehensive)

| Category | Agentic Purpose | Key Metrics | Models Needed |
|----------|-----------------|-------------|---------------|
| **Tool Use** | Execute functions, use APIs | Function call accuracy, error handling | Hermes-2-Pro, Functionary, Gorilla |
| **Structured I/O** | Agent messaging, data contracts | Schema adherence, format validity | LLaMA-3.1-8B, Mistral-v0.3, Qwen2.5 |
| **Planning** | Task decomposition, orchestration | DAG validity, dependency correctness | Orca-2, Phi-4-14B, Solar |
| **Reasoning** | Causal chains, multi-hop inference | Chain validity, discovery rate | DeepSeek-R1, WizardLM, Phi-3-128K |
| **Conversation** | User interaction, dialogue state | Multi-turn coherence, intent tracking | Vicuna, Dolphin, OpenChat |
| **Code** | Technical implementation, debugging | HumanEval, code correctness | Qwen2.5-Coder, DeepSeek-Coder, CodeLlama |
| **Embedding** | Semantic search, RAG | MTEB scores, retrieval accuracy | BGE-M3, Nomic-Embed, MXBai |
| **Communication** | Inter-agent protocols | Protocol adherence, negotiation | OpenHermes, Neural-Chat, Starling |
| **Memory** | Long context, state management | Context utilization, retrieval | LLaMA-3.1-128K, Phi-3-128K, Qwen2.5-32K |
| **Multimodal** | Vision-language, UI understanding | VQA scores, diagram interpretation | LLaVA, BakLLaVA, Granite-Vision |
| **Domain** | Specialized knowledge, compliance | Domain accuracy, terminology | Meditron, FinGPT, Biomistral |
| **Meta-Learning** | Self-improvement, adaptation | Calibration, meta-cognitive awareness | DeepSeek-R1, Phi-4, Orca-2 |

---

## Chrysalis Four-Mode Mapping

### Mode 1: Process Manager & Orchestrator

**Required Categories**:
1. Planning & Orchestration (PRIMARY)
2. Tool Use & Function Calling (PRIMARY)
3. Memory Management (SECONDARY)

**Top Models**:
- Orca-2-13B (planning)
- Hermes-2-Pro-7B (tool use)
- LLaMA-3.1-8B (context management)

**Current Coverage**: ‚ö†Ô∏è Partial - have llama3.1 but missing planning and tool use specialists

### Mode 2: Compliance Evaluator & Standards Analyst

**Required Categories**:
1. Structured Output (PRIMARY)
2. Domain Specialists (PRIMARY)
3. Multi-Hop Reasoning (SECONDARY)

**Top Models**:
- Qwen2.5-7B (structured analysis)
- Meditron-7B / FinGPT-7B (domain expertise)
- DeepSeek-R1-7B (gap identification reasoning)

**Current Coverage**: ‚ö†Ô∏è Weak - have structured output but **zero domain specialists**

### Mode 3: Root Cause Analyst & Deep Research

**Required Categories**:
1. Multi-Hop Reasoning (PRIMARY)
2. Memory Management (PRIMARY)
3. Inter-Agent Communication (SECONDARY)

**Top Models**:
- DeepSeek-R1-7B (reasoning chains)
- Phi-3-128K (long-context research)
- WizardLM-13B (complex reasoning)

**Current Coverage**: ‚ö†Ô∏è Partial - have small reasoning models, missing larger variants

### Mode 4: Meta-Process Designer & Synthesis

**Required Categories**:
1. Planning & Orchestration (PRIMARY)
2. Inter-Agent Communication (PRIMARY)
3. Structured Output (PRIMARY)
4. Meta-Learning (SECONDARY)

**Top Models**:
- Phi-4-14B (meta-cognitive reasoning)
- Neural-Chat-7B (multi-agent synthesis)
- OpenHermes-7B (protocol design)

**Current Coverage**: ‚ùå **Critical Gap** - missing all inter-agent communication specialists

---

## Implementation Priority Matrix

### Immediate (Blocks Core Functionality)

| Model | Size | Category | Mode Impact | Download Priority |
|-------|------|----------|-------------|-------------------|
| Hermes-2-Pro-Mistral-7B | 4.1GB | Function Calling | Mode 1, 4 | üî¥ P0 |
| Functionary-7B-v2 | 4.2GB | Tool Use | Mode 1 | üî¥ P0 |
| DeepSeek-R1-7B | 4.5GB | Reasoning | Mode 3 | üî¥ P0 |
| OpenHermes-7B-v2.5 | 4.3GB | Communication | Mode 4 | üî¥ P0 |

**Total**: ~17GB, 4 models

### High Priority (Enhances Evaluation Quality)

| Model | Size | Category | Mode Impact | Download Priority |
|-------|------|----------|-------------|-------------------|
| Orca-2-13B | 7.4GB | Planning | Mode 1, 3, 4 | üü° P1 |
| Qwen2.5-Coder-7B | 4.7GB | Code + Structured | Mode 1, 2 | üü° P1 |
| WizardLM-13B | 7.4GB | Reasoning | Mode 3, 4 | üü° P1 |
| Neural-Chat-7B | 4.2GB | Multi-Agent | Mode 4 | üü° P1 |

**Total**: ~24GB, 4 models

### Medium Priority (Completes Coverage)

| Model | Size | Category | Mode Impact | Download Priority |
|-------|------|----------|-------------|-------------------|
| Phi-4-14B | 8.0GB | Planning | Mode 4 | üü¢ P2 |
| Vicuna-13B-v1.5 | 7.4GB | Conversation | All Modes | üü¢ P2 |
| LLaVA-Mistral-7B | 4.7GB | Multimodal | Future | üü¢ P2 |
| Solar-10.7B | 6.1GB | Planning | Mode 1, 4 | üü¢ P2 |

**Total**: ~26GB, 4 models

### Low Priority (Nice-to-Have)

| Model | Size | Category | Mode Impact | Download Priority |
|-------|------|----------|-------------|-------------------|
| Starling-7B | 4.3GB | Communication | Mode 4 | üîµ P3 |
| BakLLaVA-7B | 4.8GB | Multimodal | Future | üîµ P3 |
| Meditron-7B | 4.1GB | Domain (Medical) | Mode 2 | üîµ P3 |
| Yi-Coder-9B | 5.5GB | Code (Multilingual) | Mode 1, 2 | üîµ P3 |

**Total**: ~19GB, 4 models

---

## Cumulative Resource Requirements

| Phase | Models | Size | Cumulative | Purpose |
|-------|--------|------|------------|---------|
| **Current** | 21 local | ~90GB | 90GB | Baseline |
| **+P0** | +4 critical | +17GB | 107GB | Minimum agentic functionality |
| **+P1** | +4 high-priority | +24GB | 131GB | Enhanced evaluation quality |
| **+P2** | +4 medium | +26GB | 157GB | Complete coverage |
| **+P3** | +4 low-priority | +19GB | 176GB | Comprehensive benchmark |

**Storage Recommendation**: Allocate **200GB** for model storage (176GB + buffer).

---

## Model Family Tracking (Strategic Awareness)

### Families to Watch (Innovation Trajectory)

1. **DeepSeek Series** (Reasoning Focus)
   - Current: DeepSeek-R1, DeepSeek-Coder
   - Watch: DeepSeek-V3 (released, tracking performance)
   - Innovation: Reinforcement learning for reasoning, MoE scaling
   - Agentic Value: Self-correcting reasoning critical for Mode 3

2. **Qwen Series** (Multilingual + Code)
   - Current: Qwen2.5, Qwen3
   - Watch: Qwen2.5-Coder specialized variants
   - Innovation: Extended context (128k), code-instruct fusion
   - Agentic Value: Global deployment, repository-scale reasoning

3. **Phi Series** (Compact High-Performance)
   - Current: Phi-3, Phi-4
   - Watch: Phi-4 variants, Phi-Vision
   - Innovation: Data quality > quantity, long context in small models
   - Agentic Value: Edge deployment, low-latency agents

4. **Mistral Family** (Efficient Architectures)
   - Current: Mistral-7B, Mistral-Nemo, Mixtral
   - Watch: Mistral-Large quantized variants
   - Innovation: Sliding window attention, MoE efficiency
   - Agentic Value: Inference speed, function calling

5. **Open-Source Collaborative** (Community Innovation)
   - Families: Nous (Hermes), OpenChat, Neural-Chat
   - Watch: New instruction datasets, RLAIF techniques
   - Innovation: Open alignment methods, preference tuning
   - Agentic Value: Customizable for specific agent roles

---

## Architectural Novelty Assessment

### Emerging Architectures Worth Tracking

1. **Mixture-of-Experts (MoE)**
   - **Examples**: Mixtral-8x7B, DeepSeek-V3
   - **Agentic Benefit**: Specialist routing = natural multi-agent architecture
   - **Challenge**: Large total parameters (but sparse activation)
   - **Status**: Mixtral-8x7B available but >8GB (26GB Q4)

2. **Depth-Upscaling**
   - **Examples**: Solar-10.7B (32 layers from Llama-2)
   - **Agentic Benefit**: More reasoning layers without width scaling
   - **Challenge**: Novel architecture, less community support
   - **Status**: Recommended for Phase 2

3. **Extended Context Architectures**
   - **Examples**: Phi-3-128K, LLaMA-3.1-128K, Qwen2.5-128K
   - **Agentic Benefit**: Handle entire conversation histories, large documents
   - **Challenge**: Context utilization efficiency varies
   - **Status**: Partially covered (llama3.1, phi3)

4. **Reasoning-Enhanced Training**
   - **Examples**: DeepSeek-R1, Phi-4-Reasoning, O1-preview
   - **Agentic Benefit**: Explicit reasoning traces for transparency
   - **Challenge**: Slower inference due to thinking tokens
   - **Status**: Have small models, need 7B+ variants

---

## Synthesis: Optimal Model Portfolio

### Minimum Viable Portfolio (P0 + Current)

**Total**: 25 models, ~107GB

**Coverage**:
- ‚úÖ Code Generation (5 models)
- ‚úÖ General Purpose (7 models)
- ‚úÖ Embeddings (7 models)
- ‚úÖ Function Calling (3 models) ‚Üê NEW
- ‚ö†Ô∏è Reasoning (4 models - small)
- ‚ö†Ô∏è Planning (1 model)
- ‚ùå Inter-Agent Communication (1 model) ‚Üê GAP REMAINS

**Evaluation Capability**: **65% complete** - can evaluate Modes 1, 2, 3 with limitations; Mode 4 partially blocked

### Recommended Portfolio (P0 + P1 + Current)

**Total**: 29 models, ~131GB

**Coverage**:
- ‚úÖ Code Generation (7 models)
- ‚úÖ General Purpose (7 models)
- ‚úÖ Embeddings (7 models)
- ‚úÖ Function Calling (3 models)
- ‚úÖ Reasoning (6 models)
- ‚úÖ Planning (3 models)
- ‚ö†Ô∏è Inter-Agent Communication (2 models)

**Evaluation Capability**: **85% complete** - all modes evaluable with good coverage

### Comprehensive Portfolio (P0 + P1 + P2 + Current)

**Total**: 33 models, ~157GB

**Coverage**:
- ‚úÖ All categories with 3+ models each
- ‚úÖ Architectural diversity across families
- ‚úÖ Quantization comparison capabilities
- ‚úÖ Multimodal processing

**Evaluation Capability**: **95% complete** - production-ready evaluation framework

---

## Model Selection Criteria Weightings (Chrysalis-Specific)

### Optimized for Four-Mode Evaluation

| Criterion | Weight | Rationale |
|-----------|--------|-----------|
| **Function Calling Reliability** | 15% | Critical for Mode 1 process execution |
| **Structured Output Quality** | 12% | Essential for inter-mode data flow |
| **Multi-Hop Reasoning** | 12% | Core requirement for Mode 3 analysis |
| **Planning Capability** | 10% | Necessary for Mode 1 and Mode 4 |
| **Context Window Size** | 10% | Multi-turn agent interactions |
| **Inference Speed** | 10% | Real-time process management (Mode 1) |
| **Tool Use Proficiency** | 8% | Agent action execution |
| **Inter-Agent Communication** | 8% | Mode 4 synthesis and coordination |
| **Memory Footprint** | 5% | Deployment viability |
| **License Permissiveness** | 5% | Commercial deployment |
| **Active Maintenance** | 3% | Long-term sustainability |
| **API Compatibility** | 2% | Integration ease |

**Total**: 100%

### Scoring Formula

```
model_score = 
    0.15 * function_calling_accuracy +
    0.12 * structured_output_quality +
    0.12 * reasoning_depth_score +
    0.10 * planning_capability +
    0.10 * (context_window_kb / 128) +  // Normalized to 128k
    0.10 * (tokens_per_second / 50) +   // Normalized to 50 t/s
    0.08 * tool_use_score +
    0.08 * communication_protocol_score +
    0.05 * (1 - memory_gb / 8) +        // Inverse - smaller better
    0.05 * license_score +
    0.03 * maintenance_score +
    0.02 * api_compatibility_score
```

---

## Cross-Reference: Agentic Design Patterns

### ReAct (Reasoning + Acting)

**Requirements**:
- Generate reasoning traces
- Execute tool calls
- Integrate observations
- Iterate based on results

**Optimal Models**:
1. Hermes-2-Pro-7B (9.5/10) - Best tool integration
2. DeepSeek-R1-7B (9.2/10) - Strong reasoning traces
3. Functionary-7B (8.9/10) - Reliable tool execution

**Current Coverage**: ‚ùå 0/3 top models installed

### ReWOO (Reasoning WithOut Observation)

**Requirements**:
- Upfront planning
- Deferred execution
- Parallel tool usage
- Result aggregation

**Optimal Models**:
1. Orca-2-13B (9.1/10) - Planning excellence
2. Phi-4-14B (8.9/10) - Systematic decomposition
3. Solar-10.7B (8.6/10) - Multi-step planning

**Current Coverage**: ‚ùå 0/3 top models installed

### Plan-and-Execute

**Requirements**:
- High-level planning
- Execution monitoring
- Dynamic replanning
- Resource allocation

**Optimal Models**:
1. Orca-2-13B (9.3/10) - Best planner
2. Phi-4-14B (8.8/10) - Adaptive planning
3. WizardLM-13B (8.5/10) - Complex instructions

**Current Coverage**: ‚ö†Ô∏è Partial - have phi4-mini (smaller variant)

### Reflection (Self-Critique)

**Requirements**:
- Self-evaluation
- Error detection
- Iterative improvement
- Confidence calibration

**Optimal Models**:
1. DeepSeek-R1-7B (9.4/10) - Self-verification built-in
2. Phi-4-14B (8.7/10) - Meta-cognitive awareness
3. WizardLM-13B (8.4/10) - Self-critique capability

**Current Coverage**: ‚ö†Ô∏è Partial - have deepseek-r1:1.5b, phi4-mini

### Multi-Agent Debate

**Requirements**:
- Role consistency
- Argument construction
- Consensus building
- Disagreement handling

**Optimal Models**:
1. Vicuna-13B (9.0/10) - Best multi-agent coherence
2. Neural-Chat-7B (8.8/10) - Collaborative training
3. OpenHermes-7B (8.6/10) - Protocol adherence

**Current Coverage**: ‚ùå 0/3 top models installed

---

## Underrepresented Strategic Families

### 1. **Nous Research Ecosystem**
**Current Representation**: None  
**Strategic Value**: Leading in function calling, instruction tuning  
**Models to Track**:
- Hermes-2-Pro series (function calling)
- Nous-Hermes-2-Mixtral (MoE + tools)
- Hermes-3-Llama-3.1 (latest flagship)

**Why Track**: Community-driven innovation in agentic capabilities, open datasets

### 2. **Berkeley Function Calling (BFCL) Models**
**Current Representation**: None  
**Strategic Value**: Academic benchmark leaders in tool use  
**Models to Track**:
- Functionary series
- Gorilla-LLM
- ToolLlama

**Why Track**: Research-backed function calling, rigorous evaluation

### 3. **Microsoft Phi Family**
**Current Representation**: Partial (phi3:3.8b, phi4-mini)  
**Strategic Value**: Compact high-performance, reasoning focus  
**Models to Track**:
- Phi-4-14B (reasoning flagship)
- Phi-Vision (multimodal)
- Future Phi releases

**Why Track**: Best quality-per-parameter ratio, strong planning

### 4. **xAI Grok** (Future)
**Current Representation**: None (not yet open-sourced)  
**Strategic Value**: Reasoning-focused, real-time knowledge  
**Models to Track**:
- Grok-2 (if open-sourced)
- Grok-mini variants

**Why Track**: Cutting-edge reasoning, potential agentic innovations

### 5. **Anthropic Claude Distillations** (Future)
**Current Representation**: None (proprietary)  
**Strategic Value**: Alignment, helpfulness, harmlessness  
**Models to Track**:
- Any open distillations or knowledge transfers
- Community fine-tunes targeting Claude behavior

**Why Track**: Gold-standard alignment for agentic systems

---

## Final Recommendations

### Executive Decision Matrix

| Decision Scenario | Recommended Action | Models to Prioritize |
|-------------------|-------------------|---------------------|
| **Budget: Minimal, Timeline: Immediate** | P0 only (4 models, 17GB) | Hermes-2-Pro, Functionary, DeepSeek-R1, OpenHermes |
| **Budget: Moderate, Timeline: Standard** | P0 + P1 (8 models, 41GB) | Add Orca-2, Qwen2.5-Coder, WizardLM, Neural-Chat |
| **Budget: Generous, Timeline: Comprehensive** | P0 + P1 + P2 (12 models, 67GB) | Add Phi-4-14B, Vicuna, LLaVA, Solar |
| **Research Focus** | All priorities + experimental | Add domain specialists, quantization variants |

### Success Metrics Post-Implementation

**Mode 1 Evaluation Capability**:
- Before: 40% (missing tool use, planning)
- After P0: 75% (have tool use, partial planning)
- After P1: 95% (complete coverage)

**Mode 2 Evaluation Capability**:
- Before: 60% (have structured output, missing domain)
- After P0: 65% (minimal improvement)
- After P1: 85% (improved reasoning)
- After P2: 90% (domain specialists needed separately)

**Mode 3 Evaluation Capability**:
- Before: 50% (small reasoning models only)
- After P0: 80% (larger reasoning models)
- After P1: 95% (complete reasoning coverage)

**Mode 4 Evaluation Capability**:
- Before: 30% (critical gaps in communication, planning)
- After P0: 70% (have communication, partial planning)
- After P1: 95% (complete coverage)

**Overall Framework Completeness**:
- **Current**: 45%
- **After P0**: 72%
- **After P1**: 88%
- **After P2**: 96%

---

## Next Steps

1. ‚úÖ **Approve Priority 0 Models** - Validate the 4 critical downloads
2. **Verify Disk Space** - Ensure 67GB available (41GB for P0+P1, with buffer)
3. **Execute Phase 1 Downloads** - Install P0 models
4. **Validate Integration** - Test each model with simple prompts
5. **Generate Task JSONs** - Create evaluation tasks for each model
6. **Run Baseline Tests** - Establish performance baselines
7. **Proceed with P1** - If P0 successful, continue with high-priority models
8. **Document Results** - Update taxonomy with empirical findings

---

**Document Version**: 1.0.0  
**Status**: Research Complete - Awaiting Implementation Approval  
**Next Action**: Switch to Code mode to implement P0 model downloads and task JSON generation
