# Chrysalis Team 4: Data & Machine Learning - CrewAI Configuration
# Mission: Implement memory system evolution—embeddings, vector indexing, persistence
# Timeline: Phase 2 (Weeks 7-12)
# Dependencies: Requires Team 1 (core platform) complete

crew_configuration:
  name: "Data & Machine Learning"
  process: "sequential"  # ML Engineer → Vector DB Engineer → Performance Engineer
  verbose: true
  memory: true
  cache: true
  planning: true
  
  success_metrics:
    - "Semantic embeddings functional (@xenova/transformers)"
    - "HNSW indexing operational (O(log N) search)"
    - "LanceDB persistence working"
    - "Memory scales to 100K+ memories"
    - "Search latency <50ms p99"
    - "Accuracy >95% for semantic similarity"

# =============================================================================
# AGENTS
# =============================================================================

agents:
  - id: "ml_engineer_embeddings"
    role: "ML Engineer - Embeddings Specialist"
    goal: |
      Integrate semantic embeddings into Chrysalis memory system using
      @xenova/transformers. Implement EmbeddingService, optimize for
      performance (caching, batching), and validate semantic similarity
      improvements over Jaccard baseline.
    
    backstory: |
      ML engineer with deep expertise in transformer models, sentence embeddings,
      and semantic similarity. Specialist in @xenova/transformers (Transformers.js),
      model optimization, and vector operations. Previously built semantic search
      for 10M+ documents. Expert in all-MiniLM-L6-v2, sentence-transformers,
      ONNX Runtime, and cosine similarity. Strong knowledge of embedding model
      selection, dimensionality (384-dim optimal for most), and performance
      optimization (batching, caching, quantization). Contributor to Transformers.js.
      Believes embeddings are the key to semantic understanding.
    
    verbose: true
    allow_delegation: false
    
    tools:
      - name: "ml_model_loader"
        description: "Load and test ML models"
      
      - name: "embedding_benchmark"
        description: "Benchmark embedding performance"
      
      - name: "similarity_evaluator"
        description: "Evaluate similarity metrics"
      
      - name: "model_optimizer"
        description: "Optimize model for production"
    
    memory_enabled: true
    max_iterations: 20

  - id: "vector_db_engineer"
    role: "Data Engineer - Vector Databases & HNSW"
    goal: |
      Implement HNSW vector indexing for O(log N) memory search and integrate
      LanceDB for persistent vector storage. Optimize index parameters,
      validate recall, and ensure production-grade performance.
    
    backstory: |
      Data engineer specializing in vector databases, HNSW algorithm, and
      semantic search. Expert in LanceDB, Qdrant, Milvus, and Weaviate.
      Deep knowledge of approximate nearest neighbor search, index optimization
      (M, efConstruction parameters), and query performance. Previously built
      semantic search infrastructure for 100M+ vectors. Specialist in HNSW
      (Hierarchical Navigable Small World), IVF (Inverted File Index), and
      PQ (Product Quantization). Contributor to hnswlib and LanceDB.
      Believes vector search is the foundation of semantic AI.
    
    verbose: true
    allow_delegation: false
    
    tools:
      - name: "vector_db_client"
        description: "Test vector database operations"
      
      - name: "index_optimizer"
        description: "Optimize HNSW parameters"
      
      - name: "query_analyzer"
        description: "Analyze query performance"
      
      - name: "recall_evaluator"
        description: "Measure recall accuracy"
    
    memory_enabled: true
    max_iterations: 20

  - id: "performance_engineer"
    role: "Performance Optimization Engineer"
    goal: |
      Benchmark all memory system configurations (Jaccard, embedding, HNSW),
      identify bottlenecks, optimize critical paths, and validate performance
      targets using open source profiling and load testing tools.
    
    backstory: |
      Performance engineer with 9+ years in profiling, optimization, and
      load testing. Expert in k6, Clinic.js, Chrome DevTools, and Node.js
      performance. Deep knowledge of V8 optimization, async performance,
      memory profiling, and CPU profiling. Specialist in identifying hot
      paths, reducing allocations, and optimizing algorithms. Previously
      optimized systems to handle 100K+ RPS. Contributor to k6 and
      performance testing tools. Believes "you can't improve what you
      don't measure."
    
    verbose: true
    allow_delegation: false
    
    tools:
      - name: "profiler"
        description: "CPU and memory profiling"
      
      - name: "load_test_runner"
        description: "Run k6 load tests"
      
      - name: "bottleneck_analyzer"
        description: "Identify performance bottlenecks"
      
      - name: "optimizer"
        description: "Apply optimization techniques"
    
    memory_enabled: true
    max_iterations: 20

# =============================================================================
# TASKS
# =============================================================================

tasks:
  # ===== ML Engineer Tasks =====
  
  - id: "TASK-4.1.1"
    description: |
      Integrate @xenova/transformers into Chrysalis memory system:
      
      Implementation Requirements:
      1. EmbeddingService class:
         - initialize(): Load model (all-MiniLM-L6-v2)
         - embed(text: string): Promise<number[]> (384-dim vector)
         - batchEmbed(texts: string[]): Promise<number[][]>
         - cosineSimilarity(emb1, emb2): number (range [0, 1])
         - isReady(): boolean
      
      2. Performance Optimizations:
         - Model caching (load once, reuse)
         - Embedding caching (LRU cache, 10K embeddings)
         - Batch processing (32 texts per batch)
         - Lazy initialization (on first use)
      
      3. Configuration:
         - Model selection (default: all-MiniLM-L6-v2)
         - Cache size (default: 10K)
         - Batch size (default: 32)
      
      Benchmark: embedding generation latency, batch speedup, accuracy vs Jaccard.
    
    agent: "ml_engineer_embeddings"
    expected_output: |
      Complete embedding integration:
      - src/memory/EmbeddingService.ts (~400-500 lines)
      - src/memory/EmbeddingCache.ts (~200 lines)
      - tests/memory/EmbeddingService.test.ts (~300 lines)
      - benchmarks/embedding-performance.ts
      - benchmarks/results/embedding-benchmarks.md
      - docs/technical/EMBEDDINGS.md
    
    output_file: "src/memory/EmbeddingService.ts"
    
    technology_requirements:
      dependencies:
        - name: "@xenova/transformers"
          license: "Apache 2.0"
          version: "2.x"
          purpose: "Browser/Node.js ML, sentence embeddings"
          why_chosen: "No Python dependency, runs in Node.js, good performance"
          model: "Xenova/all-MiniLM-L6-v2 (384 dimensions)"
        
        - name: "lru-cache"
          license: "ISC"
          purpose: "LRU cache for embeddings"
          why_chosen: "Fast, battle-tested, simple API"
        
        - name: "onnxruntime-node"
          license: "MIT"
          purpose: "ONNX runtime (fallback if needed)"
      
      algorithms:
        - name: "Sentence Embedding"
          model: "all-MiniLM-L6-v2"
          input: "Text string"
          output: "Float32Array[384]"
          complexity: "O(N) for N tokens"
          latency_target: "<50ms p99"
        
        - name: "Cosine Similarity"
          formula: "dot(a,b) / (norm(a) * norm(b))"
          complexity: "O(D) for D dimensions (384)"
          range: "[-1, 1] normalized to [0, 1]"
    
    acceptance_criteria:
      - "Model loads successfully (~100MB download)"
      - "Embeddings generated correctly (384-dim)"
      - "Cosine similarity accurate"
      - "Performance: <50ms per embedding"
      - "Batch processing 10x+ faster than serial"
      - "Cache hit rate >60% (typical workload)"
    
    priority: "CRITICAL"
    estimated_hours: 32
    dependencies: []

  - id: "TASK-4.1.2"
    description: |
      Update MemoryMerger to use EmbeddingService for semantic similarity:
      
      Changes Required:
      1. Add to MemoryMergerConfig:
         - similarity_method: 'jaccard' | 'embedding'
         - embedding_service?: EmbeddingService
      
      2. Implement embeddingSimilarity():
         - Generate embeddings for both texts
         - Compute cosine similarity
         - Return similarity score [0, 1]
      
      3. Update calculateSimilarity():
         - Dispatch to jaccardSimilarity() or embeddingSimilarity()
         - Based on config.similarity_method
      
      4. Maintain backward compatibility:
         - Default: 'jaccard' (no breaking changes)
         - Embedding mode: opt-in via config
      
      Benchmark accuracy improvement: test with 100 paraphrase pairs,
      measure similarity scores (expect >0.9 for paraphrases vs <0.5 for Jaccard).
    
    agent: "ml_engineer_embeddings"
    expected_output: |
      Enhanced MemoryMerger:
      - src/experience/MemoryMerger.ts (updated, ~500 lines)
      - tests/experience/MemoryMerger.embedding.test.ts (~250 lines)
      - benchmarks/similarity-comparison.ts
      - benchmarks/results/jaccard-vs-embedding.md
      - docs/technical/SEMANTIC_SIMILARITY.md
    
    output_file: "src/experience/MemoryMerger.ts"
    context: ["src/experience/MemoryMerger.ts (current)", "src/memory/EmbeddingService.ts"]
    
    technology_requirements:
      algorithm_changes:
        - "Add embeddingSimilarity() method"
        - "Update calculateSimilarity() dispatcher"
        - "Add embedding generation to memory processing"
      
      testing:
        accuracy_tests:
          - "Test with 100 paraphrase pairs (STSB dataset sample)"
          - "Measure Jaccard similarity (baseline)"
          - "Measure embedding similarity (improved)"
          - "Expected: Embedding >0.9 for paraphrases, Jaccard <0.5"
        
        performance_tests:
          - "100 memories: Jaccard vs Embedding latency"
          - "1000 memories: Scalability comparison"
    
    acceptance_criteria:
      - "Embedding mode functional"
      - "Jaccard mode still works (backward compat)"
      - "Accuracy improvement >30% (measured)"
      - "Performance acceptable (<100ms for 100 memories)"
      - "Configuration clear and documented"
    
    priority: "CRITICAL"
    estimated_hours: 24
    dependencies: ["TASK-4.1.1", "TASK-1.2.4 from Team 1"]

  # ===== Vector DB Engineer Tasks =====
  
  - id: "TASK-4.2.1"
    description: |
      Implement HNSW vector indexing using hnswlib-node:
      
      Implementation:
      1. MemoryVectorIndex class:
         - initialize(dimensions: number, space: 'cosine'): Promise<void>
         - addMemory(id: string, embedding: number[]): Promise<void>
         - search(queryEmbedding: number[], topK: number): Promise<SearchResult[]>
         - delete(id: string): Promise<void>
         - save(path: string): Promise<void> (persist index)
         - load(path: string): Promise<void> (load from disk)
         - getStats(): IndexStats (size, memory usage)
      
      2. HNSW Parameter Optimization:
         - M: 16 (connections per node, balance speed vs recall)
         - efConstruction: 200 (construction quality)
         - efSearch: 50 (search quality)
         - Test recall at various ef values
      
      3. Integration with MemoryMerger:
         - Option: use_vector_index: boolean in config
         - If true: O(log N) search via HNSW
         - If false: O(N) linear scan
      
      Benchmark: Search latency vs index size (100, 1K, 10K, 100K vectors),
      measure recall@10 (target >95%).
    
    agent: "vector_db_engineer"
    expected_output: |
      Complete HNSW implementation:
      - src/memory/VectorIndex.ts (~350 lines)
      - src/memory/VectorIndex.test.ts (~250 lines)
      - benchmarks/hnsw-performance.ts
      - benchmarks/results/hnsw-scaling.md
      - benchmarks/results/recall-analysis.md
      - docs/technical/VECTOR_INDEX.md
    
    output_file: "src/memory/VectorIndex.ts"
    
    technology_requirements:
      dependencies:
        - name: "hnswlib-node"
          license: "Apache 2.0"
          purpose: "HNSW algorithm for approximate nearest neighbor"
          why_chosen: "Fast, reliable, good Node.js bindings"
          alternatives: ["faiss-node (limited)", "annoy (older)", "custom HNSW (complex)"]
      
      algorithms:
        - name: "HNSW Construction"
          complexity: "O(log N) per insertion"
          memory: "O(N * M) edges"
        
        - name: "HNSW Search"
          complexity: "O(log N) search"
          recall: ">95% @ top-10 (typical)"
          latency: "<5ms for 100K vectors"
      
      parameter_optimization:
        M:
          range: "8-32"
          recommended: "16"
          tradeoff: "Higher M = better recall, more memory"
        
        efConstruction:
          range: "100-500"
          recommended: "200"
          tradeoff: "Higher ef = better index quality, slower build"
        
        efSearch:
          range: "10-500"
          recommended: "50"
          tradeoff: "Higher ef = better recall, slower search"
    
    acceptance_criteria:
      - "HNSW index builds successfully"
      - "Search is O(log N) (empirically verified)"
      - "Handles 100K+ vectors"
      - "Recall >95% @ top-10"
      - "Search latency <10ms p99"
      - "Persistence (save/load) works"
    
    priority: "CRITICAL"
    estimated_hours: 40
    dependencies: ["TASK-4.1.1"]

  - id: "TASK-4.2.2"
    description: |
      Integrate LanceDB for persistent vector storage:
      
      Implementation:
      1. MemoryPersistenceLayer class:
         - connect(path: string): Promise<void>
         - createTable(name: string, schema): Promise<void>
         - saveMemory(memory: Memory, embedding: number[]): Promise<void>
         - loadMemories(filter?): Promise<Memory[]>
         - searchSimilar(embedding: number[], topK: number): Promise<Memory[]>
         - updateMemory(id: string, updates): Promise<void>
         - deleteMemory(id: string): Promise<void>
      
      2. Schema Design:
         - memory_id (UUID, primary key)
         - agent_id (UUID, indexed)
         - memory_type (episodic | semantic)
         - content (TEXT)
         - embedding (VECTOR[384])
         - metadata (JSON)
         - created_at (TIMESTAMP)
         - accessed_count (INTEGER)
      
      3. Features:
         - ACID transactions
         - Vector similarity search (native)
         - Filtering (by agent_id, type, date range)
         - Versioning (LanceDB native feature)
      
      Benchmark: Write throughput, read latency, search performance vs HNSW-only.
    
    agent: "vector_db_engineer"
    expected_output: |
      Complete LanceDB integration:
      - src/memory/Persistence.ts (~400 lines)
      - src/memory/schema.ts (LanceDB schema definitions)
      - tests/memory/Persistence.test.ts (~300 lines)
      - benchmarks/lancedb-performance.ts
      - benchmarks/results/lancedb-vs-memory.md
      - docs/technical/PERSISTENCE.md
    
    output_file: "src/memory/Persistence.ts"
    
    technology_requirements:
      dependencies:
        - name: "vectordb"
          license: "Apache 2.0"
          package: "vectordb (LanceDB Node.js client)"
          purpose: "Persistent vector database"
          why_chosen: "Embedded, Apache Arrow, excellent Node.js support, ACID"
          alternatives: ["Qdrant (needs server)", "Chroma (Python-first)", "Weaviate (complex)"]
      
      features:
        - "Embedded (no separate server)"
        - "Apache Arrow (efficient columnar storage)"
        - "ACID transactions"
        - "Native vector search"
        - "Versioning (git-like)"
        - "SQL-like query interface"
    
    acceptance_criteria:
      - "LanceDB connection functional"
      - "CRUD operations work"
      - "Vector search functional"
      - "Data persists across restarts"
      - "Performance: Write <5ms, Read <10ms, Search <50ms p99"
      - "ACID properties verified"
    
    priority: "CRITICAL"
    estimated_hours: 40
    dependencies: ["TASK-4.2.1"]

  - id: "TASK-4.2.3"
    description: |
      Integrate VectorIndex and Persistence with MemoryMerger:
      
      Integration Points:
      1. MemoryMergerConfig additions:
         - use_vector_index: boolean
         - vector_index?: MemoryVectorIndex
         - persistence?: MemoryPersistenceLayer
      
      2. MemoryMerger.findDuplicate() optimization:
         - If vector index available: O(log N) search
         - Else: O(N) linear scan (current)
      
      3. MemoryMerger.addMemory():
         - Generate embedding
         - Check for duplicates (via index if available)
         - Save to persistence (if configured)
         - Add to index
      
      4. MemoryMerger.initialize():
         - Load existing memories from persistence
         - Build vector index from loaded memories
      
      End-to-end testing: Create agent with 10K memories, search, verify O(log N).
    
    agent: "vector_db_engineer"
    expected_output: |
      Complete integration:
      - src/experience/MemoryMerger.ts (updated, ~600 lines)
      - tests/experience/MemoryMerger.integration.test.ts (~400 lines)
      - examples/memory-system-advanced.ts
      - docs/technical/MEMORY_INTEGRATION.md
    
    output_file: "src/experience/MemoryMerger.ts"
    
    acceptance_criteria:
      - "Vector index integration works"
      - "Persistence integration works"
      - "findDuplicate() uses O(log N) search"
      - "Memories survive restarts"
      - "Tests with 10K memories pass"
      - "Performance: <50ms p99 for search"
    
    priority: "CRITICAL"
    estimated_hours: 32
    dependencies: ["TASK-4.2.2", "TASK-4.1.2"]

  # ===== Performance Engineer Tasks =====
  
  - id: "TASK-4.3.1"
    description: |
      Comprehensive memory system benchmarking:
      
      Benchmark Scenarios:
      1. Similarity Computation:
         - Jaccard similarity (100, 1K memories)
         - Embedding similarity (100, 1K, 5K memories)
         - Comparison: Accuracy and latency
      
      2. Memory Search:
         - Linear scan (100, 1K memories)
         - HNSW index (1K, 10K, 100K memories)
         - Measure: Latency, recall, memory usage
      
      3. Batch Embedding:
         - Serial embedding (10, 100 texts)
         - Batch embedding (10, 100 texts, batch_size=32)
         - Measure: Speedup factor
      
      4. End-to-End Merge:
         - Merge 10, 100, 1000 memories
         - Measure: Total time, memory usage
         - Compare: Jaccard vs Embedding vs HNSW
      
      5. Persistence:
         - Write 10K memories (throughput)
         - Read 10K memories (latency)
         - Search 10K memories (latency + recall)
      
      Generate graphs: latency vs scale, accuracy vs method, throughput vs batch size.
    
    agent: "performance_engineer"
    expected_output: |
      Comprehensive benchmark results:
      - benchmarks/memory-system/*.ts (5+ benchmark scripts)
      - benchmarks/results/similarity-comparison.md
      - benchmarks/results/search-scaling.md
      - benchmarks/results/embedding-performance.md
      - benchmarks/results/persistence-performance.md
      - benchmarks/results/end-to-end.md
      - graphs/ (10+ performance graphs)
      - PERFORMANCE_REPORT.md (executive summary)
    
    output_file: "benchmarks/"
    
    technology_requirements:
      tools:
        - name: "k6"
          license: "AGPL v3"
          purpose: "Load testing"
        
        - name: "Clinic.js"
          license: "Apache 2.0"
          purpose: "Node.js profiling"
          alternatives: ["0x (MIT)", "Chrome DevTools"]
        
        - name: "autocannon"
          license: "MIT"
          purpose: "HTTP benchmarking"
    
    acceptance_criteria:
      - "All 5 benchmark categories complete"
      - "Jaccard: confirmed <1K memory limit"
      - "Embedding: confirmed <5K memory limit"
      - "HNSW: confirmed 100K+ memory scale"
      - "Graphs clearly show scaling behavior"
      - "Performance report published"
    
    priority: "HIGH"
    estimated_hours: 40
    dependencies: ["TASK-4.2.3"]

  - id: "TASK-4.3.2"
    description: |
      Optimize critical paths based on profiling:
      
      Optimization Targets:
      1. Memory Merging:
         - Reduce allocations in similarity computation
         - Optimize embedding caching (hit rate >80%)
         - Parallelize batch operations
      
      2. Vector Search:
         - Tune HNSW parameters (M, ef)
         - Optimize query path
         - Reduce memory copies
      
      3. Persistence:
         - Batch writes (reduce I/O)
         - Connection pooling
         - Query optimization
      
      Measure: Before/after optimization, target 30% improvement.
    
    agent: "performance_engineer"
    expected_output: |
      Optimization results:
      - src/memory/*.ts (optimized, inline comments)
      - benchmarks/optimization-results.md
      - docs/technical/PERFORMANCE_OPTIMIZATIONS.md
    
    acceptance_criteria:
      - "Critical paths optimized"
      - "30%+ improvement measured"
      - "No functionality regression"
    
    priority: "MEDIUM"
    estimated_hours: 32
    dependencies: ["TASK-4.3.1"]

# =============================================================================
# EXECUTION PLAN
# =============================================================================

execution_plan:
  week_7:
    - "TASK-4.1.1: ML engineer integrates @xenova/transformers (32h)"
  
  week_8:
    - "TASK-4.1.2: ML engineer updates MemoryMerger (24h)"
    - "TASK-4.2.1: Vector DB engineer implements HNSW (starts, 40h total)"
  
  week_9:
    - "TASK-4.2.1: Vector DB engineer completes HNSW (40h total)"
    - "TASK-4.2.2: Vector DB engineer integrates LanceDB (starts, 40h total)"
  
  week_10:
    - "TASK-4.2.2: Vector DB engineer completes LanceDB (40h total)"
    - "TASK-4.2.3: Vector DB engineer integrates with MemoryMerger (32h)"
  
  week_11:
    - "TASK-4.3.1: Performance engineer benchmarks (40h)"
  
  week_12:
    - "TASK-4.3.2: Performance engineer optimizes (32h)"
    - "Final validation and documentation"

# =============================================================================
# DELIVERABLES SUMMARY
# =============================================================================

deliverables_summary:
  code_modules:
    - "src/memory/EmbeddingService.ts (~500 lines)"
    - "src/memory/VectorIndex.ts (~350 lines)"
    - "src/memory/Persistence.ts (~400 lines)"
    - "src/experience/MemoryMerger.ts (enhanced, ~600 lines)"
  
  benchmarks:
    - "benchmarks/memory-system/ (8+ benchmark scripts)"
    - "benchmarks/results/ (5+ result documents)"
    - "graphs/ (10+ performance graphs)"
  
  documentation:
    - "docs/technical/EMBEDDINGS.md"
    - "docs/technical/VECTOR_INDEX.md"
    - "docs/technical/PERSISTENCE.md"
    - "docs/technical/MEMORY_INTEGRATION.md"
    - "PERFORMANCE_REPORT.md"
  
  total_lines_of_code: "~4,000 lines (code + tests + benchmarks)"

# =============================================================================
# SUCCESS CRITERIA
# =============================================================================

success_criteria:
  functionality:
    - "Semantic embeddings functional"
    - "HNSW indexing operational"
    - "LanceDB persistence working"
    - "Full integration with MemoryMerger"
  
  performance:
    - "Embedding: <50ms p99"
    - "HNSW search: <10ms p99 for 100K vectors"
    - "LanceDB operations: <50ms p99"
    - "End-to-end merge: <100ms p99 for 1K memories"
  
  scalability:
    - "Handles 100K+ memories"
    - "Search is O(log N) (measured)"
    - "Memory usage reasonable (<2GB for 100K vectors)"
  
  accuracy:
    - "Semantic similarity >90% for paraphrases"
    - "HNSW recall >95% @ top-10"
    - "30%+ improvement over Jaccard"

# =============================================================================
# INTEGRATION WITH TEAM 1
# =============================================================================

team1_integration:
  enhances:
    - "MemoryMerger with semantic similarity"
    - "Memory system with O(log N) search"
    - "Memory system with persistence"
  
  requires:
    - "MemoryMerger base implementation (TASK-1.2.4)"
    - "Memory interfaces (Episode, Concept)"
    - "UniversalAgentV2 schema"
  
  provides:
    - "Production-grade memory system"
    - "Scalability to millions of memories"
    - "Semantic understanding of content"

# =============================================================================
# OPEN SOURCE TECHNOLOGY EVALUATION
# =============================================================================

technology_evaluation:
  embeddings:
    selected: "@xenova/transformers"
    license: "Apache 2.0"
    community_health:
      github_stars: "~5K+"
      recent_activity: "Very active (daily commits)"
      maintainers: "Xenova + Hugging Face team"
      verdict: "Healthy, growing rapidly"
    
    why_chosen:
      - "No Python dependency (runs in Node.js)"
      - "Good performance (~50ms per embedding)"
      - "Hugging Face model ecosystem"
      - "Active development"
    
    alternatives_considered:
      - name: "Python transformers + gRPC server"
        pros: "More model options, better performance"
        cons: "Complex (multiple runtimes), operational overhead"
        verdict: "Too complex for embedded use case"
      
      - name: "OpenAI Embeddings API"
        pros: "Best quality, no local compute"
        cons: "Not open source, cost, latency, privacy"
        verdict: "Against open source principles"

  vector_index:
    selected: "hnswlib-node"
    license: "Apache 2.0"
    community_health:
      github_stars: "~300"
      recent_activity: "Moderate (monthly commits)"
      maintainers: "2-3 active"
      verdict: "Stable, mature"
    
    why_chosen:
      - "HNSW is state-of-art for ANN"
      - "Good Node.js bindings"
      - "Fast and reliable"
      - "Apache 2.0 license"
    
    alternatives:
      - "faiss-node (Facebook AI): Limited bindings, complex"
      - "annoy (Spotify): Older, less maintained"
      - "Custom HNSW: Too complex to implement"

  vector_database:
    selected: "LanceDB"
    license: "Apache 2.0"
    community_health:
      github_stars: "~2K+"
      recent_activity: "Very active"
      maintainers: "10+ active (backed by company)"
      verdict: "Healthy, rapid innovation"
    
    why_chosen:
      - "Embedded (no separate server)"
      - "Apache Arrow (efficient)"
      - "ACID transactions"
      - "Excellent Node.js support"
      - "Versioning built-in"
      - "Apache 2.0 license"
    
    alternatives:
      - name: "Qdrant"
        license: "Apache 2.0"
        pros: "High performance, rich filtering"
        cons: "Requires server (not embedded)"
        verdict: "Good for MCP fabric model, overkill for embedded"
      
      - name: "Weaviate"
        license: "BSD 3-Clause"
        pros: "Feature-rich, GraphQL"
        cons: "Complex, requires server"
        verdict: "Too complex for current needs"
      
      - name: "Chroma"
        license: "Apache 2.0"
        pros: "Simple, Python-native"
        cons: "Python-first (limited Node.js)"
        verdict: "Not ideal for TypeScript project"

# =============================================================================
# RISK MITIGATION
# =============================================================================

risks:
  - risk: "@xenova/transformers performance insufficient (<50ms target)"
    probability: "LOW"
    impact: "HIGH"
    mitigation: "Benchmark early, have fallback (simpler model or ONNX Runtime)"
    owner: "ml_engineer_embeddings"
  
  - risk: "HNSW recall <95% (accuracy insufficient)"
    probability: "LOW"
    impact: "MEDIUM"
    mitigation: "Tune parameters (increase M, ef), measure carefully"
    owner: "vector_db_engineer"
  
  - risk: "LanceDB stability issues (new library)"
    probability: "MEDIUM"
    impact: "HIGH"
    mitigation: "Extensive testing, monitor GitHub issues, have fallback (Qdrant)"
    owner: "vector_db_engineer"

---

# Team 4 Status: READY FOR EXECUTION (After Team 1 Complete)
# Total Estimated Effort: ~240 hours (3 agents * 6 weeks, weeks 7-12)
# Critical Path: ML Engineer → Vector DB Engineer → Performance Engineer
# Blocking on: Team 1 completion (needs MemoryMerger)
