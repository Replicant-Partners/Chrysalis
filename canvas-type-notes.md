I need a set of prompts which can be used as a uniform test to score the logical, semantic, rocessual and metacognitive capabilities of a set of small local LLMs.  I will want to score the LLMs on response time and also accuracy or functionality of responses.  The role which the LLMs will be in will be as the coach or manager of a virtual continuous improvement kata logic machine which is intended to perform tasks but also reflect on how processes and tasks can be improved by simulatenously acting in 4 diferent modes or roles: (1) manager - running and calibrating the ongoing process; (2) process analyst - evaluating the existing process and its performance and implementation against indiustry best practices as defined in a registry or at an external web link where the best practices and protocols are being updated (like the w3c or best practices for collaborating on github or using a platform that is part of a process: https://docs.github.com/en/organizations/collaborating-with-groups-in-organizations ); (3) deep research and root cause analyst -- drilling down into any problems asking why and looking out to adjacent context elements and digging deeper in existing resources to see if there are better registries or resources to anchors to; (4) complex learner -- the mode in agent.md -- which synthesizes all thee prvious three to ask if there is a better way to (a) manage the process by (b) using any of the registries or resources found in the root cause analysis and deep research to ask different questions or use different prompts.  The synthesis step is focused on evaluating information found by mode-3 in mode-4 to use in modes-&2.  So the task is to define a prompt set to score small LLM capabilities to perform roles 1, 2, 3, or 4 of this nuerosymbolic logic engine -- which is implemented as the "universal adapter and task framework" in the chrysalis project.



please form running the evaluation into a series of task json files, where you create a series of json task files for each of the ollama local LLMs I have installed that are between 1GB and 8GB in size (you will need to run ollama list and parse that to get the current list -- I downloaded several more this morning) -- all with the prompt set and a process diagram for going through them and the output from the task should be a markdown file which will be scored.  make versions of the test to be run by the leading major LLMs as well for us to be able to score them or use them as the benchmark -- this woudl be Sonnet 4.5, Opus 4.5, OpenAI-GPT5.2-codex and OpenAIGPT5.2, Z.ai GLM4.7 andthe latest top thinking Mistral model and others that might come up in your our research to check on the root cause accuracy and deep research on adjacent opportunities.  The conclusion of the task in th task JSON should be to save the responses to the prompts in an output folder.  Please as a final step, please design a script to pass the universal adapter this the task jsons through the addhoc task cli interface (please test and confirm the interface is connected and working before submitting the test -- so there will be a test of the adhoc interface before we use it to pass the tests to the adapter).  And then run the script to run the tests and let me know what we learn.



The user interaction pattern would be that a user would launch a canvas and then send a token to one of the TUI chat panes for the collaborators in that pane to be able to pick up and join the user in that canvas collaborating in real time.  The tokens could be either view only or read/write in the canvas, or full acess (can read write in the canvas and push commands that use local system resources on the users machine).  The users would grab the token and go to open a canvas and use the token to sign into or open the canvas the token was associated with.  This is a new specific definition of the sharing mode which we had stated as a requiremen but hadn't specifically defined.  For this first release of the software -- sharing is this user initiated and by chat session via a token that is only good for that session which is shared through the chat pane when it is created.  Does this make sense?  what am I missing in the scenario? what other questions  do we need to answer or is that enough for now for us to get this first layer built?



Review and reconstruct the set of merge options after adding two key considerations that seem to have been missed: (1) the merge would be a task for the universal adapter  -- so it's not if its agentic, but is the merge just a task for the adapter or does it need to be mediated or overseen or checked by a system agent too; (2) the platform is designed to be low cognitive load so that the users can focus on their work, so the last thing we would do is demand the user manage additional workflows like a user managed complex merge decision tree -- so the merge cannot involve the user in doing anything beyond choosing to merge and agreeing/accepting a method -- note the mege is a handshake because all the users need to agree.  I think this distributed handshake was missed too so thats a 3rd item.   We are creating a powerful platform with both capabilities (the embedded adapter and systems agents) and also limits (hard stops on complexity in the UI and cognitive load demands that are platform specific and made on the users).





you nailed what I was thinking on the immutabe task architecture -- so thats solved from my perspective.   Kind of knew you would nail it.  Now - on the prompt set, the first question is with the architecture of the DND and the scheduling of the task and the architecture of the universal adapter.  As I understand the adapter, a task is called from the code and then the adapter runs it.  So the adapter will run the syncconflictresolution-XYZ-canvas task when the canvas calls for it, and the canvas will call for it when the CRDTprocess runs into a merge conflict.  So the way i imagined the architecture and code working was that the DND button just stops the canvas from asking the Adapter to run the syncconflictresolution and turns off the CRDT process until it gets clicked off and then there would be a surge of updates and probable conflicts.  So this now gets us to what the prompts need to contain principles to handle --  conflict resolution when there are two changes that cannot both be accepted as is.  And this is why its also a social contract issue.  There are an infinite number of ways to handle this group merger issue-- but what's nice is that there is only one method or set of principles that makes sense to me:  1. All collaborators are not the same - some collaborators are also admins and at least one collaborator, normally the originaor of the canvas, has to be an admin.  The admin has two superpowers: to rollback changes and also to lock widgets, or objects within the canvs from any further change. Conflicts between admin changes are different too - in the case of a conflict between admins the clock tells you the winner - so the winning change is the first one or the winner of the race. 2. In the case of conflicts between changes made by collaborators, they are dealt with collaboratively and with mutual respect as though the collaborators got together and found the middle ground between their conflicting changes.  3. So the conflict resolution task is a prompt or set of prompts designed to perform a context sensitive decompositon and reconstruction, or a "semantic merge" a I will sometimes call it.  Logically - what this liteally involves is decomposing each change into RDF using the relevant ontology for the space the users are working in and what they are trying to do -- so decomposing the two conflicting entities into their smallest logical constituents and then combining those sets of rdf statements and eliminating duplicates and the recomposing or reconstituting whatever that builds.  So the prompt set could be this really cool step by step logical decomposion and recombination guide that could be useful in a lot of ways in a lot of contexts.  Thoughts?





1. settings is for managing the system and making sure you know where you files are and you are connected to the right tools and you can see how the terminal configured and then connected to your local system and also how it is connected to cloud resources and llms. 2. for scrapbook, the mode is to collect material on a topic -- but you don't know how to organize it yet - so the key is the ease of linking information or artifacts or dropping them into the canvas, and then making associations or grouping items and also attaching note to items on their significance and then being able to come back and sort and query and reflect on and re-assemble and find some way to organize or make sense of what you gather. 3. for research -- you know what you want to know and you know how to make sense of it -- but you need to go get the data or perspectives and artifacts and that will be multi-modal -- what is interesting is that logically the use pattern in scrapbooking and research is the same sort of gathering pattern -- but in one case the domain and sense-making methods are part of what is being discovered (scrapbooking) whereas in the other setting the domain and sense-making frameworks are actualy shaping and defining the gathering process. 4. Wiki is probably the simplest -- from a format perspective we just want to use the same backend as wikiedia, but use this framework to store and build the way we make sense of what we find in the research and scrapbooking processes which we want to preserve and build into an internal or shared knowledgebase in some way -- the wiki is a knowledgebase for the user and his/her collaborators.  in the context of the ai agents, its a way for the ai agents to aggregate and structure the empirical knowledge they gather.  The wiki canvas is expected to be useful to the system and internal agents as a way of storing and building and sharing their explicit knowledge.  5. terminal/browser is a mode in which teams can be coding or working through interfaces that have either coding or internet/external world connectivity and can be a useful place to get work done together in front of each other.  this is kind of like a study group around a library table.

   

   the agent canvas is a place for the human-in -the-loop user of the chrysalis terminal to manage their internal agents -- to store, revise, maintain, and also run teams of agents.  while you could keep as many agents as you wanted in a single canvas on infinite scroll -- the expected use case is to have canvases that contain an active team of agents who are working with you on a project.  the key innovation is that with the wiki's and the memory stack and other future accomodations -- the chrysalis project is intended to enable the agents to gather and refine capabilities, knowledge and skills, such that working with agents in the chrysalis terminal will become a competitive advantage to the human-in-the-loop users of the chrysalis terminal.