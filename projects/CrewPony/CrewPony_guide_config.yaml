# Guide LLM System Configuration
# Configure available models and their settings

default_model: ollama-llama2

models:
  # Local Ollama Models
  ollama-llama2:
    provider: ollama
    model_name: llama2
    capabilities:
      - chat
      - completion
    context_window: 4096
    max_tokens: 2000
    temperature: 0.7
    endpoint: http://localhost:11434

  ollama-codellama:
    provider: ollama
    model_name: codellama
    capabilities:
      - code
      - completion
    context_window: 4096
    max_tokens: 2000
    temperature: 0.3
    endpoint: http://localhost:11434

  ollama-mistral:
    provider: ollama
    model_name: mistral
    capabilities:
      - chat
      - completion
      - reasoning
    context_window: 8192
    max_tokens: 4000
    temperature: 0.5
    endpoint: http://localhost:11434

  ollama-mixtral:
    provider: ollama
    model_name: mixtral
    capabilities:
      - chat
      - completion
      - code
      - reasoning
    context_window: 32768
    max_tokens: 8000
    temperature: 0.5
    endpoint: http://localhost:11434

  # OpenAI Models
  gpt-4:
    provider: openai
    model_name: gpt-4
    capabilities:
      - chat
      - completion
      - code
      - reasoning
      - function_calling
    context_window: 8192
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.00003
    latency_estimate: 50

  gpt-4-turbo:
    provider: openai
    model_name: gpt-4-turbo-preview
    capabilities:
      - chat
      - completion
      - code
      - reasoning
      - vision
      - function_calling
    context_window: 128000
    max_tokens: 4096
    temperature: 0.7
    cost_per_token: 0.00001
    latency_estimate: 40

  gpt-3.5-turbo:
    provider: openai
    model_name: gpt-3.5-turbo
    capabilities:
      - chat
      - completion
      - function_calling
    context_window: 16385
    max_tokens: 4096
    temperature: 0.7
    cost_per_token: 0.000002
    latency_estimate: 20

  # Anthropic Models
  claude-3-opus:
    provider: anthropic
    model_name: claude-3-opus-20240229
    capabilities:
      - chat
      - completion
      - code
      - reasoning
      - vision
    context_window: 200000
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.000015
    latency_estimate: 60

  claude-3-sonnet:
    provider: anthropic
    model_name: claude-3-sonnet-20240229
    capabilities:
      - chat
      - completion
      - code
      - reasoning
    context_window: 200000
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.000003
    latency_estimate: 30

  claude-3-haiku:
    provider: anthropic
    model_name: claude-3-haiku-20240307
    capabilities:
      - chat
      - completion
    context_window: 200000
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.00000025
    latency_estimate: 15

# Model Selection Rules
selection_rules:
  # Priority order for capability matching
  capability_priority:
    code:
      - ollama-codellama
      - gpt-4
      - claude-3-opus
    reasoning:
      - claude-3-opus
      - gpt-4
      - ollama-mixtral
    vision:
      - gpt-4-turbo
      - claude-3-opus
    function_calling:
      - gpt-4
      - gpt-4-turbo
      - gpt-3.5-turbo
    
  # Cost optimization settings
  cost_optimization:
    enabled: true
    max_cost_per_request: 0.10
    prefer_local: true
    
  # Latency optimization
  latency_optimization:
    enabled: true
    max_latency_ms: 5000
    prefer_fast_models: true

# Agent-specific configurations
agent_configs:
  researcher:
    preferred_models:
      - claude-3-opus
      - gpt-4
    required_capabilities:
      - reasoning
    temperature_override: 0.3
    
  coder:
    preferred_models:
      - ollama-codellama
      - gpt-4
    required_capabilities:
      - code
    temperature_override: 0.2
    
  analyst:
    preferred_models:
      - claude-3-sonnet
      - gpt-4
    required_capabilities:
      - reasoning
    temperature_override: 0.5
    
  manager:
    preferred_models:
      - gpt-3.5-turbo
      - ollama-llama2
    required_capabilities:
      - chat
    temperature_override: 0.7

# Fallback chain
fallback_chain:
  - ollama-llama2
  - ollama-mistral
  - gpt-3.5-turbo
  - claude-3-haiku

# Rate limiting
rate_limits:
  openai:
    requests_per_minute: 60
    tokens_per_minute: 90000
  anthropic:
    requests_per_minute: 50
    tokens_per_minute: 100000
  ollama:
    requests_per_minute: 100
    tokens_per_minute: -1  # No limit

# Cache settings
cache:
  enabled: true
  ttl_seconds: 3600
  max_entries: 1000
  
# Logging
logging:
  level: INFO
  file: guide_llm.log
  rotate_size_mb: 100
  keep_backups: 5